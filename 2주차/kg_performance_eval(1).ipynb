{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8e31f4",
   "metadata": {},
   "source": [
    "# KG ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜ (ROUGE ê¸°ë°˜ í‰ê°€)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d3a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "kg = pd.read_csv(\"kg_triples_test.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# íŠ¸ë¦¬í”Œ ê²°í•© í…ìŠ¤íŠ¸ ë§Œë“¤ê¸°\n",
    "kg[\"triple_text\"] = kg[\"subject\"] + \" \" + kg[\"predicate\"] + \" \" + kg[\"object\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac6233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê¸°ë°˜ KG ê²€ìƒ‰ í•¨ìˆ˜\n",
    "def find_relevant_kg(text, kg_texts, top_k=3):\n",
    "    matches = []\n",
    "    for t in kg_texts:\n",
    "        score = sum([1 for w in t.split() if w in text])\n",
    "        matches.append((t, score))\n",
    "    matches.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [x[0] for x in matches[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55bbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ êµ¬ì„± í•¨ìˆ˜\n",
    "def build_prompt(text, kg_hits):\n",
    "    context = \"\\n\".join(kg_hits)\n",
    "    return f\"ë‹¤ìŒì€ ì°¸ê³  ì§€ì‹ì…ë‹ˆë‹¤:\\n{context}\\n\\nì‚¬ìš©ì ì…ë ¥: {text}\\në‹µë³€:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d974c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE í‰ê°€ í•¨ìˆ˜\n",
    "def evaluate_with_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, pred)[\"rougeL\"].fmeasure for pred, ref in zip(predictions, references)]\n",
    "    return sum(scores) / len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a94795",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'input'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m sample = test_df.sample(\u001b[32m10\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1. KG ì—†ì´ (baseline)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m preds_no_kg = [\u001b[33m\"\u001b[39m\u001b[33më‹µë³€: \u001b[39m\u001b[33m\"\u001b[39m + text[:\u001b[32m20\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. KG ì‚¬ìš©\u001b[39;00m\n\u001b[32m     10\u001b[39m preds_with_kg = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'input'"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# ìƒ˜í”Œ 10ê°œ ì¶”ì¶œ\n",
    "sample = test_df.sample(10, random_state=42)\n",
    "\n",
    "# 1. KG ì—†ì´ (baseline)\n",
    "preds_no_kg = [\"ë‹µë³€: \" + text[:20] for text in sample[\"input\"]]\n",
    "\n",
    "# 2. KG ì‚¬ìš©\n",
    "preds_with_kg = []\n",
    "for text in sample[\"input\"]:\n",
    "    hits = find_relevant_kg(text, kg[\"triple_text\"])\n",
    "    prompt = build_prompt(text, hits)\n",
    "    preds_with_kg.append(\"ë‹µë³€: \" + text[:20] + \" (ì§€ì‹ë°˜ì˜)\")\n",
    "\n",
    "# í‰ê°€\n",
    "rouge_no_kg = evaluate_with_rouge(preds_no_kg, sample[\"output\"])\n",
    "rouge_with_kg = evaluate_with_rouge(preds_with_kg, sample[\"output\"])\n",
    "\n",
    "print(f\"KG ë¯¸ì‚¬ìš© ì‹œ ROUGE-L: {rouge_no_kg:.4f}\")\n",
    "print(f\"KG ì‚¬ìš© ì‹œ ROUGE-L: {rouge_with_kg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22f11a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì§ˆë¬¸: ë¼ì´ìŠ¤ í˜ì´í¼ì˜ ë‘ê»˜ë‚˜ êµ¬ì„±ì´ ì¼ë°˜ì ì´ì§€ ì•Šì•„ ì¡°ê¸ˆ ë†€ëìŠµë‹ˆë‹¤. ì•„ë§ˆ, ì‹ì‚¬ëŸ‰ì„ ì¶©ë¶„íˆ í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜€ì„ê±´ë°, ì´ë¡œ ì¸í•´ ì›”ë‚¨ìŒˆì˜ ì§ˆê°ì´ë‚˜ ë§›ì„ ëŠë¼ëŠ” ë¶€ë¶„ì—ì„œ ì¡°ê¸ˆ í˜ë“¤ì—ˆìŠµë‹ˆë‹¤. ê°œì¸ì ìœ¼ë¡œëŠ” í› ê¶ˆ ìœ¡ìˆ˜ëŠ” ì¡°ê¸ˆ ì•„ì‰¬ì›€ì´ ë‚¨ì•˜ì§€ë§Œ, ë‹¤ë¥¸ ë©”ë‰´ë“¤ ì¤‘ì—ì„œëŠ” ê³ ê¸°ê°€ ë¹¼ì–´ë‚œ ë§›ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n",
      "\n",
      "[í”„ë¡¬í”„íŠ¸ - KG ë¯¸ì‚¬ìš©]\n",
      " ì§ˆë¬¸: ë¼ì´ìŠ¤ í˜ì´í¼ì˜ ë‘ê»˜ë‚˜ êµ¬ì„±ì´ ì¼ë°˜ì ì´ì§€ ì•Šì•„ ì¡°ê¸ˆ ë†€ëìŠµë‹ˆë‹¤. ì•„ë§ˆ, ì‹ì‚¬ëŸ‰ì„ ì¶©ë¶„íˆ í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜€ì„ê±´ë°, ì´ë¡œ ì¸í•´ ì›”ë‚¨ìŒˆì˜ ì§ˆê°ì´ë‚˜ ë§›ì„ ëŠë¼ëŠ” ë¶€ë¶„ì—ì„œ ì¡°ê¸ˆ í˜ë“¤ì—ˆìŠµë‹ˆë‹¤. ê°œì¸ì ìœ¼ë¡œëŠ” í› ê¶ˆ ìœ¡ìˆ˜ëŠ” ì¡°ê¸ˆ ì•„ì‰¬ì›€ì´ ë‚¨ì•˜ì§€ë§Œ, ë‹¤ë¥¸ ë©”ë‰´ë“¤ ì¤‘ì—ì„œëŠ” ê³ ê¸°ê°€ ë¹¼ì–´ë‚œ ë§›ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n",
      "ë‹µë³€:\n",
      "\n",
      "[í”„ë¡¬í”„íŠ¸ - KG ì‚¬ìš©]\n",
      " ì§ˆë¬¸: ë¼ì´ìŠ¤ í˜ì´í¼ì˜ ë‘ê»˜ë‚˜ êµ¬ì„±ì´ ì¼ë°˜ì ì´ì§€ ì•Šì•„ ì¡°ê¸ˆ ë†€ëìŠµë‹ˆë‹¤. ì•„ë§ˆ, ì‹ì‚¬ëŸ‰ì„ ì¶©ë¶„íˆ í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜€ì„ê±´ë°, ì´ë¡œ ì¸í•´ ì›”ë‚¨ìŒˆì˜ ì§ˆê°ì´ë‚˜ ë§›ì„ ëŠë¼ëŠ” ë¶€ë¶„ì—ì„œ ì¡°ê¸ˆ í˜ë“¤ì—ˆìŠµë‹ˆë‹¤. ê°œì¸ì ìœ¼ë¡œëŠ” í› ê¶ˆ ìœ¡ìˆ˜ëŠ” ì¡°ê¸ˆ ì•„ì‰¬ì›€ì´ ë‚¨ì•˜ì§€ë§Œ, ë‹¤ë¥¸ ë©”ë‰´ë“¤ ì¤‘ì—ì„œëŠ” ê³ ê¸°ê°€ ë¹¼ì–´ë‚œ ë§›ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n",
      "ë°°ê²½ì§€ì‹:\n",
      "- ì‚¬ìš©ì ì§ˆë¬¸í•œë‹¤ ë‹¤ë¥¸ ì‚¬ëŒì˜ ì˜ë„\n",
      "ë‹µë³€:\n",
      "\n",
      "ğŸ¤– [KG ë¯¸ì‚¬ìš© ë‹µë³€]\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] ì§€ì •ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# âœ… 8. Ollama ì‹¤í–‰ ë° ì¶œë ¥ ê²°ê³¼\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ¤– [KG ë¯¸ì‚¬ìš© ë‹µë³€]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrun_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_no_kg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ¤– [KG ì‚¬ìš© ë‹µë³€]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(run_ollama(prompt_kg))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mrun_ollama\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_ollama\u001b[39m(prompt, model=\u001b[33m\"\u001b[39m\u001b[33mllama3\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     result = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mollama\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.stdout.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\subprocess.py:556\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstdout\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m    554\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstderr\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    558\u001b[39m         stdout, stderr = process.communicate(\u001b[38;5;28minput\u001b[39m, timeout=timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\subprocess.py:1038\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1034\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1035\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1036\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1048\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\subprocess.py:1550\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[39m\n\u001b[32m   1548\u001b[39m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     hp, ht, pid, tid = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[32m   1552\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1554\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1559\u001b[39m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[32m   1560\u001b[39m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1563\u001b[39m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[32m   1564\u001b[39m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[32m   1565\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_pipe_fds(p2cread, p2cwrite,\n\u001b[32m   1566\u001b[39m                          c2pread, c2pwrite,\n\u001b[32m   1567\u001b[39m                          errread, errwrite)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] ì§€ì •ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
     ]
    }
   ],
   "source": [
    "# âœ… 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”©\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "# âœ… 2. Knowledge Graph ë¡œë”© ë° í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "kg = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg[\"triple_text\"] = kg[\"subject\"] + \" \" + kg[\"predicate\"] + \" \" + kg[\"object\"]\n",
    "\n",
    "# âœ… 3. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë¡œë”©\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# âœ… 4. KG ê²€ìƒ‰ í•¨ìˆ˜\n",
    "def find_relevant_kg(user_input, kg_texts, topk=3):\n",
    "    return [t for t in kg_texts if any(word in user_input for word in t.split())][:topk]\n",
    "\n",
    "# âœ… 5. í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜\n",
    "def build_prompt(user_input, kg_hits=None):\n",
    "    prompt = f\"ì§ˆë¬¸: {user_input}\\n\"\n",
    "    if kg_hits:\n",
    "        prompt += \"ë°°ê²½ì§€ì‹:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += \"ë‹µë³€:\"\n",
    "    return prompt\n",
    "\n",
    "# âœ… 6. ë¡œì»¬ Ollama ëª¨ë¸ í˜¸ì¶œ í•¨ìˆ˜ (ì˜ˆ: llama3)\n",
    "def run_ollama(prompt, model=\"llama3\"):\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt.encode(\"utf-8\"),\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        timeout=30\n",
    "    )\n",
    "    return result.stdout.decode(\"utf-8\").strip()\n",
    "\n",
    "# âœ… 7. ì§ˆë¬¸ ì…ë ¥ â†’ KG ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ë¹„êµ\n",
    "sample = test_df.sample(1, random_state=42)\n",
    "user_input = sample[\"sentence1\"].values[0]\n",
    "true_answer = sample[\"sentence2\"].values[0]\n",
    "\n",
    "kg_hits = find_relevant_kg(user_input, kg[\"triple_text\"])\n",
    "prompt_kg = build_prompt(user_input, kg_hits)\n",
    "prompt_no_kg = build_prompt(user_input)\n",
    "\n",
    "print(\"ğŸ“Œ ì§ˆë¬¸:\", user_input)\n",
    "print(\"\\n[í”„ë¡¬í”„íŠ¸ - KG ë¯¸ì‚¬ìš©]\\n\", prompt_no_kg)\n",
    "print(\"\\n[í”„ë¡¬í”„íŠ¸ - KG ì‚¬ìš©]\\n\", prompt_kg)\n",
    "\n",
    "# âœ… 8. Ollama ì‹¤í–‰ ë° ì¶œë ¥ ê²°ê³¼\n",
    "print(\"\\nğŸ¤– [KG ë¯¸ì‚¬ìš© ë‹µë³€]\\n\")\n",
    "print(run_ollama(prompt_no_kg))\n",
    "\n",
    "print(\"\\nğŸ¤– [KG ì‚¬ìš© ë‹µë³€]\\n\")\n",
    "print(run_ollama(prompt_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53429b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ KG ë¯¸ì‚¬ìš©:\n",
      "í”„ë¡¬í”„íŠ¸: ì§ˆë¬¸: ì§ˆë¬¸: ë‚˜ëŠ” [MASK] ê¸°ë¶„ì´ë‹¤\n",
      "ë‹µë³€: ì§ˆë¬¸: ì§ˆë¬¸: ë‚˜ëŠ” ê½‰ ê¸°ë¶„ì´ë‹¤\n",
      "\n",
      "ğŸ“Œ KG ì‚¬ìš©:\n",
      "í”„ë¡¬í”„íŠ¸: ì§ˆë¬¸: ì§ˆë¬¸: ë‚˜ëŠ” [MASK] ê¸°ë¶„ì´ë‹¤\n",
      "ë‹µë³€: ì§ˆë¬¸: ì§ˆë¬¸: ë‚˜ëŠ” ê½‰ ê¸°ë¶„ì´ë‹¤\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import pandas as pd\n",
    "\n",
    "# CPUë¡œ ê°•ì œ ì„¤ì •\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# KoBERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertForMaskedLM.from_pretrained('monologg/kobert')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# KG ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "if 'triple_text' not in kg_df.columns:\n",
    "    kg_df['triple_text'] = kg_df['subject'] + \" \" + kg_df['predicate'] + \" \" + kg_df['object']\n",
    "\n",
    "# ë°°ê²½ì§€ì‹ ì°¾ê¸°\n",
    "def find_relevant_kg(question, kg_df):\n",
    "    hits = []\n",
    "    for triple in kg_df['triple_text']:\n",
    "        if any(word in triple for word in question.split()):\n",
    "            hits.append(triple)\n",
    "    return hits[:3]\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "def build_prompt(question, kg_hits=None):\n",
    "    prompt = \"\"\n",
    "    if kg_hits:\n",
    "        prompt += \"ë°°ê²½ì§€ì‹:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += f\"ì§ˆë¬¸: {question}\"\n",
    "    return prompt\n",
    "\n",
    "# ë‹µë³€ ìƒì„±\n",
    "def generate_answer(prompt):\n",
    "    if \"[MASK]\" not in prompt:\n",
    "        return \"ì§ˆë¬¸ì— [MASK]ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = outputs.logits[0, mask_token_index, :]\n",
    "    top_token = torch.argmax(mask_token_logits, dim=1)\n",
    "    predicted_token = tokenizer.decode(top_token)\n",
    "    \n",
    "    return prompt.replace(\"[MASK]\", predicted_token)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "user_question = input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë‚˜ëŠ” [MASK] ê¸°ë¶„ì´ë‹¤): \")\n",
    "\n",
    "# KG ë¯¸ì‚¬ìš©\n",
    "prompt_no_kg = build_prompt(user_question)\n",
    "answer_no_kg = generate_answer(prompt_no_kg)\n",
    "\n",
    "# KG ì‚¬ìš©\n",
    "kg_hits = find_relevant_kg(user_question, kg_df)\n",
    "prompt_with_kg = build_prompt(user_question, kg_hits)\n",
    "answer_with_kg = generate_answer(prompt_with_kg)\n",
    "\n",
    "print(\"\\n KG ë¯¸ì‚¬ìš©:\")\n",
    "print(\"í”„ë¡¬í”„íŠ¸:\", prompt_no_kg)\n",
    "print(\"ë‹µë³€:\", answer_no_kg)\n",
    "\n",
    "print(\"\\n KG ì‚¬ìš©:\")\n",
    "print(\"í”„ë¡¬í”„íŠ¸:\", prompt_with_kg)\n",
    "print(\"ë‹µë³€:\", answer_with_kg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ [KG ë¯¸ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\n",
      " ì§ˆë¬¸: í”¼ê³¤í•´\n",
      "ë‹µë³€:\n",
      "\n",
      "ğŸ¤– [KG ë¯¸ì‚¬ìš© ë‹µë³€]\n",
      " â±ï¸ ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Œ [KG ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\n",
      " ì§ˆë¬¸: í”¼ê³¤í•´\n",
      "ë‹µë³€:\n",
      "\n",
      "ğŸ¤– [KG ì‚¬ìš© ë‹µë³€]\n",
      " â±ï¸ ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "# Ollama ì‹¤í–‰ ê²½ë¡œ (ì§ì ‘ ì§€ì •)\n",
    "OLLAMA_PATH = r\"C:\\Users\\lsm40\\AppData\\Local\\Programs\\Ollama\\ollama.exe\"\n",
    "\n",
    "# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥\n",
    "user_input = input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# KG íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")  # ê²½ë¡œëŠ” í•„ìš”ì— ë”°ë¼ ìˆ˜ì •\n",
    "\n",
    "# KGì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰)\n",
    "def find_relevant_kg(question, kg_df):\n",
    "    hits = []\n",
    "    for idx, row in kg_df.iterrows():\n",
    "        triple = f\"{row['subject']}, {row['predicate']}, {row['object']}\"\n",
    "        if any(word in triple for word in question.split()):\n",
    "            hits.append(triple)\n",
    "    return hits[:3]  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì‚¬ìš©\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "def build_prompt(user_input, kg_hits=None):\n",
    "    prompt = \"\"\n",
    "    if kg_hits:\n",
    "        prompt += \"ë°°ê²½ì§€ì‹:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += f\"ì§ˆë¬¸: {user_input}\\në‹µë³€:\"\n",
    "    return prompt\n",
    "\n",
    "#  Llama3ë¡œ ë‹µë³€ ìƒì„±\n",
    "def run_ollama(prompt, model=\"llama3\"):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [OLLAMA_PATH, \"run\", model],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=60\n",
    "        )\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except FileNotFoundError:\n",
    "        return \" Ollama ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \" ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "    except Exception as e:\n",
    "        return f\" ì˜¤ë¥˜ ë°œìƒ: {e}\"\n",
    "\n",
    "#  KG ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "kg_hits = find_relevant_kg(user_input, kg_df)\n",
    "\n",
    "#  í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "prompt_kg = build_prompt(user_input, kg_hits)\n",
    "prompt_no_kg = build_prompt(user_input)\n",
    "\n",
    "#  ì¶œë ¥\n",
    "print(\"\\n [KG ë¯¸ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\\n\", prompt_no_kg)\n",
    "print(\"\\n [KG ë¯¸ì‚¬ìš© ë‹µë³€]\\n\", run_ollama(prompt_no_kg))\n",
    "\n",
    "print(\"\\n [KG ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\\n\", prompt_kg)\n",
    "print(\"\\n [KG ì‚¬ìš© ë‹µë³€]\\n\", run_ollama(prompt_kg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51647132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ [KG ë¯¸ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\n",
      " ì§ˆë¬¸: ë‚˜ í”¼ê³¤í•´\n",
      "ë‹µë³€:\n",
      "\n",
      "ğŸ¤– [KG ë¯¸ì‚¬ìš© ë‹µë³€]\n",
      " â±ï¸ ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Œ [KG ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\n",
      " ì§ˆë¬¸: ë‚˜ í”¼ê³¤í•´\n",
      "ë‹µë³€:\n",
      "\n",
      "ğŸ¤– [KG ì‚¬ìš© ë‹µë³€]\n",
      " â±ï¸ ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#  Ollama ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ (ì§ì ‘ í™•ì¸ëœ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)\n",
    "OLLAMA_PATH = r\"C:\\Users\\lsm40\\AppData\\Local\\Programs\\Ollama\\ollama.exe\"\n",
    "\n",
    "#  ì‚¬ìš©ì ì§ˆë¬¸ ì‹¤ì‹œê°„ ì…ë ¥\n",
    "user_input = input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "\n",
    "#  KG ê·¸ë˜í”„ CSV ë¡œë”© (ì»¬ëŸ¼ëª… ìë™ ê°ì§€)\n",
    "kg_path = \"./kg_triples_test.csv\"  # ì˜ˆ: 'triple_text' ì—´ì´ ìˆëŠ” íŒŒì¼\n",
    "kg_df = pd.read_csv(kg_path)\n",
    "col = kg_df.columns[0]  # ì²« ë²ˆì§¸ ì—´ ìë™ ì¸ì‹\n",
    "\n",
    "#  KGì—ì„œ ê´€ë ¨ëœ ë°°ê²½ì§€ì‹ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def find_relevant_kg(question, df, column):\n",
    "    hits = []\n",
    "    for triple in df[column]:\n",
    "        if any(word in triple for word in question.split()):\n",
    "            hits.append(triple)\n",
    "    return hits[:3]\n",
    "\n",
    "#  í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜\n",
    "def build_prompt(question, kg_hits=None):\n",
    "    prompt = \"\"\n",
    "    if kg_hits:\n",
    "        prompt += \"ë°°ê²½ì§€ì‹:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += f\"ì§ˆë¬¸: {question}\\në‹µë³€:\"\n",
    "    return prompt\n",
    "\n",
    "#  Llama3 ì‹¤í–‰ í•¨ìˆ˜\n",
    "def run_ollama(prompt, timeout=60):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [OLLAMA_PATH, \"run\", \"llama3\"],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        if result.stderr:\n",
    "            print(\" stderr:\", result.stderr.decode(\"utf-8\").strip())\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \" ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "    except FileNotFoundError:\n",
    "        return \" Ollama ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\"\n",
    "\n",
    "#  KG ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "kg_hits = find_relevant_kg(user_input, kg_df, col)\n",
    "prompt_no_kg = build_prompt(user_input)\n",
    "prompt_kg = build_prompt(user_input, kg_hits)\n",
    "\n",
    "#  ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n [KG ë¯¸ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\\n\", prompt_no_kg)\n",
    "print(\"\\n [KG ë¯¸ì‚¬ìš© ë‹µë³€]\\n\", run_ollama(prompt_no_kg))\n",
    "\n",
    "print(\"\\n [KG ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\\n\", prompt_kg)\n",
    "print(\"\\n [KG ì‚¬ìš© ë‹µë³€]\\n\", run_ollama(prompt_kg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e98a60b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training without KG\n",
      "Epoch 1 complete\n",
      "Epoch 2 complete\n",
      "Epoch 3 complete\n",
      "Evaluation without KG\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.00      0.00      0.00        38\n",
      "          ai       0.80      1.00      0.89       151\n",
      "\n",
      "    accuracy                           0.80       189\n",
      "   macro avg       0.40      0.50      0.44       189\n",
      "weighted avg       0.64      0.80      0.71       189\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with KG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete\n",
      "Evaluation with KG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.00      0.00      0.00        38\n",
      "          ai       0.80      1.00      0.89       151\n",
      "\n",
      "    accuracy                           0.80       189\n",
      "   macro avg       0.40      0.50      0.44       189\n",
      "weighted avg       0.64      0.80      0.71       189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "model_name = 'monologg/kobert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ë¼ë²¨ ì²˜ë¦¬\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df['label'].dtype == object:\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "df = df.dropna(subset=['label'])\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# KG ë¡œë“œ\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = [\n",
    "    f\"{r['subject']} {r['predicate']} {r['object']}\"\n",
    "    for _, r in kg_df.iterrows()\n",
    "]\n",
    "\n",
    "def find_relevant_kg(text, kg_texts, topk=3):\n",
    "    hits = [kg for kg in kg_texts if any(tok in kg for tok in text.split())]\n",
    "    return \" \".join(hits[:topk])\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['text']\n",
    "        kg = find_relevant_kg(text, kg_texts) if self.use_kg else None\n",
    "\n",
    "        # ìˆ˜ì •ëœ ë¶€ë¶„: truncation=True ë¡œ í†µì¼\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            kg,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in encoded.items()}\n",
    "        item['labels'] = torch.tensor(row['label'], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "def get_model():\n",
    "    return BertForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "def train_model(model, loader, epochs=3):\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            outputs.loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1} complete\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**batch).logits\n",
    "            preds += torch.argmax(logits, axis=1).cpu().tolist()\n",
    "            labels += batch['labels'].cpu().tolist()\n",
    "    print(classification_report(labels, preds, target_names=[\"human\", \"ai\"], zero_division=0))\n",
    "\n",
    "# KG ë¯¸ì‚¬ìš©\n",
    "model = get_model()\n",
    "train_loader = DataLoader(\n",
    "    PoetryDataset(train_df, tokenizer, use_kg=False),\n",
    "    batch_size=8, shuffle=True, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    PoetryDataset(test_df, tokenizer, use_kg=False),\n",
    "    batch_size=8, shuffle=False, pin_memory=True\n",
    ")\n",
    "print(\"Training without KG\")\n",
    "train_model(model, train_loader, epochs=3)\n",
    "print(\"Evaluation without KG\")\n",
    "evaluate(model, test_loader)\n",
    "\n",
    "# KG ì‚¬ìš©\n",
    "model = get_model()\n",
    "train_loader = DataLoader(\n",
    "    PoetryDataset(train_df, tokenizer, use_kg=True),\n",
    "    batch_size=8, shuffle=True, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    PoetryDataset(test_df, tokenizer, use_kg=True),\n",
    "    batch_size=8, shuffle=False, pin_memory=True\n",
    ")\n",
    "print(\"Training with KG\")\n",
    "train_model(model, train_loader, epochs=3)\n",
    "print(\"Evaluation with KG\")\n",
    "evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f21f6ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ë¡œë“œ í›„ ìƒ˜í”Œ ìˆ˜: 945\n",
      " ë¼ë²¨ ë¶„í¬ (ë¡œë“œ ì§í›„):\n",
      " label\n",
      "1    756\n",
      "0    189\n",
      "Name: count, dtype: int64\n",
      " ì •ì œ í›„ ìƒ˜í”Œ ìˆ˜: 945\n",
      " ë¼ë²¨ ë¶„í¬ (ì •ì œ í›„):\n",
      " label\n",
      "1    756\n",
      "0    189\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " KG ë¯¸ì‚¬ìš© í•™ìŠµ ì‹œì‘\n",
      "Epoch 1 ì™„ë£Œ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m dl_test  \u001b[38;5;241m=\u001b[39m DataLoader(PoetryDataset(test_df,  tokenizer, use_kg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    111\u001b[0m                       batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m KG ë¯¸ì‚¬ìš© í•™ìŠµ ì‹œì‘\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m KG ë¯¸ì‚¬ìš© ê²°ê³¼\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m evaluate(model, dl_test)\n",
      "Cell \u001b[0;32mIn[18], line 91\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, loader, epochs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     90\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     optim\u001b[38;5;241m.\u001b[39mstep(); optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ì™„ë£Œ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/cuda-env/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cuda-env/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cuda-env/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# 1) ë°ì´í„° ë¡œë“œ\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "print(\" ë¡œë“œ í›„ ìƒ˜í”Œ ìˆ˜:\", len(df))\n",
    "print(\" ë¼ë²¨ ë¶„í¬ (ë¡œë“œ ì§í›„):\\n\", df['label'].value_counts())\n",
    "\n",
    "# 2) ë¬¸ìì—´ 'human'/'ai' â†’ 0/1 ë§¤í•‘ (ì´ë¯¸ ìˆ«ìë©´ ê±´ë„ˆë›°ê¸°)\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df['label'].dtype == object:\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "\n",
    "# 3) ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ & 0Â·1 ì™¸ ë‚˜ë¨¸ì§€(NaN í¬í•¨) ì‚­ì œ\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce').astype('Int64')\n",
    "df = df[df['label'].isin([0, 1])].reset_index(drop=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "print(\" ì •ì œ í›„ ìƒ˜í”Œ ìˆ˜:\", len(df))\n",
    "print(\" ë¼ë²¨ ë¶„í¬ (ì •ì œ í›„):\\n\", df['label'].value_counts())\n",
    "\n",
    "# 4) KG ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = (\n",
    "    kg_df\n",
    "    .apply(lambda r: f\"{r['subject']} {r['predicate']} {r['object']}\", axis=1)\n",
    "    .tolist()\n",
    ")\n",
    "def find_relevant_kg(text, kg_texts, topk=3):\n",
    "    hits = [kg for kg in kg_texts if any(tok in kg for tok in text.split())]\n",
    "    return \" \".join(hits[:topk])\n",
    "\n",
    "# 5) Dataset ì •ì˜\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, use_kg=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        if self.use_kg:\n",
    "            kg = find_relevant_kg(text, kg_texts)\n",
    "            text = f\"{text} [SEP] {kg}\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.df.iloc[idx]['label'], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# 6) ë°ì´í„° ë¶„ë¦¬\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "# 7) í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì¤€ë¹„\n",
    "model_name = 'monologg/kobert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cpu\")\n",
    "def get_model():\n",
    "    return BertForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "# 8) í•™ìŠµ/í‰ê°€ í•¨ìˆ˜\n",
    "def train_model(model, loader, epochs=3):\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "    model.train()\n",
    "    for e in range(epochs):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            loss = model(**batch).loss\n",
    "            loss.backward()\n",
    "            optim.step(); optim.zero_grad()\n",
    "        print(f\"Epoch {e+1} ì™„ë£Œ\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**batch).logits\n",
    "            preds += torch.argmax(logits, axis=1).cpu().tolist()\n",
    "            labs  += batch['labels'].cpu().tolist()\n",
    "    print(classification_report(labs, preds, target_names=['human','ai']))\n",
    "\n",
    "# 9) KG ë¯¸ì‚¬ìš©\n",
    "model = get_model()\n",
    "dl_train = DataLoader(PoetryDataset(train_df, tokenizer, use_kg=False),\n",
    "                      batch_size=8, shuffle=True)\n",
    "dl_test  = DataLoader(PoetryDataset(test_df,  tokenizer, use_kg=False),\n",
    "                      batch_size=8)\n",
    "print(\" KG ë¯¸ì‚¬ìš© í•™ìŠµ ì‹œì‘\")\n",
    "train_model(model, dl_train)\n",
    "print(\" KG ë¯¸ì‚¬ìš© ê²°ê³¼\")\n",
    "evaluate(model, dl_test)\n",
    "\n",
    "# 10) KG ì‚¬ìš©\n",
    "model = get_model()\n",
    "dl_train = DataLoader(PoetryDataset(train_df, tokenizer, use_kg=True),\n",
    "                      batch_size=8, shuffle=True)\n",
    "dl_test  = DataLoader(PoetryDataset(test_df,  tokenizer, use_kg=True),\n",
    "                      batch_size=8)\n",
    "print(\"\\n KG ì‚¬ìš© í•™ìŠµ ì‹œì‘\")\n",
    "train_model(model, dl_train)\n",
    "print(\" KG ì‚¬ìš© ê²°ê³¼\")\n",
    "evaluate(model, dl_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62522105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ìµœì¢… ìƒ˜í”Œ ìˆ˜: 945\n",
      " ë¼ë²¨ ë¶„í¬:\n",
      " label\n",
      "1    756\n",
      "0    189\n",
      "Name: count, dtype: int64\n",
      " Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ì‹¤í—˜ 1] KG ë¯¸ì‚¬ìš© í•™ìŠµ ì‹œì‘\n",
      "Epoch 1/5 ì™„ë£Œ\n",
      "Epoch 2/5 ì™„ë£Œ\n",
      "Epoch 3/5 ì™„ë£Œ\n",
      "Epoch 4/5 ì™„ë£Œ\n",
      "Epoch 5/5 ì™„ë£Œ\n",
      "\n",
      " KG ë¯¸ì‚¬ìš© í‰ê°€ ê²°ê³¼\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.41      0.79      0.54        38\n",
      "          ai       0.93      0.72      0.81       151\n",
      "\n",
      "    accuracy                           0.73       189\n",
      "   macro avg       0.67      0.75      0.67       189\n",
      "weighted avg       0.83      0.73      0.76       189\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ì‹¤í—˜ 2] KG ì‚¬ìš© í•™ìŠµ ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 ì™„ë£Œ\n",
      "Epoch 2/5 ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 ì™„ë£Œ\n",
      "Epoch 5/5 ì™„ë£Œ\n",
      "\n",
      " KG ì‚¬ìš© í‰ê°€ ê²°ê³¼\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.36      0.82      0.50        38\n",
      "          ai       0.93      0.64      0.76       151\n",
      "\n",
      "    accuracy                           0.67       189\n",
      "   macro avg       0.65      0.73      0.63       189\n",
      "weighted avg       0.82      0.67      0.70       189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import json\n",
    "\n",
    "# 1) ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# 2) ë°ì´í„° ë¶ˆëŸ¬ì™€ì„œ DataFrame ìƒì„±\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 3) ë¼ë²¨ ì •ì œ\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df['label'].dtype == object:\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce').astype('Int64')\n",
    "df = df[df['label'].isin([0, 1])].reset_index(drop=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "print(\" ìµœì¢… ìƒ˜í”Œ ìˆ˜:\", len(df))\n",
    "print(\" ë¼ë²¨ ë¶„í¬:\\n\", df['label'].value_counts())\n",
    "\n",
    "# 4) KG ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = [\n",
    "    f\"{r['subject']} {r['predicate']} {r['object']}\"\n",
    "    for _, r in kg_df.iterrows()\n",
    "]\n",
    "\n",
    "def find_relevant_kg(text, kg_texts, topk=3):\n",
    "    hits = [kg for kg in kg_texts if any(tok in kg for tok in text.split())]\n",
    "    return \" \".join(hits[:topk])\n",
    "\n",
    "# 5) Dataset ì •ì˜\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, 'text']\n",
    "        kg_text = find_relevant_kg(text, kg_texts) if self.use_kg else None\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            kg_text,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            truncation='only_first' if kg_text else True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.df.loc[idx, 'label'], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# 6) train/test split\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "# 7) GPU ì„¸íŒ…\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Using device:\", device)\n",
    "\n",
    "# 8) tokenizer & model í•¨ìˆ˜\n",
    "model_name = 'monologg/kobert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def get_model():\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# 9) Oversampling sampler\n",
    "counts = train_df['label'].value_counts().sort_index().tolist()  # [#human, #ai]\n",
    "sample_weights = [1.0 / counts[label] for label in train_df['label']]\n",
    "sampler = WeightedRandomSampler(\n",
    "    sample_weights, num_samples=len(sample_weights), replacement=True\n",
    ")\n",
    "\n",
    "# 10) DataLoader ìƒì„± í•¨ìˆ˜\n",
    "def make_train_loader(df, use_kg):\n",
    "    return DataLoader(\n",
    "        PoetryDataset(df, tokenizer, use_kg=use_kg),\n",
    "        batch_size=8,\n",
    "        sampler=sampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "def make_test_loader(df, use_kg):\n",
    "    return DataLoader(\n",
    "        PoetryDataset(df, tokenizer, use_kg=use_kg),\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "train_loader_no_kg = make_train_loader(train_df, use_kg=False)\n",
    "test_loader_no_kg  = make_test_loader(test_df,  use_kg=False)\n",
    "train_loader_kg    = make_train_loader(train_df, use_kg=True)\n",
    "test_loader_kg     = make_test_loader(test_df,  use_kg=True)\n",
    "\n",
    "# 11) í•™ìŠµ/í‰ê°€ í•¨ìˆ˜\n",
    "def train_model(model, loader, epochs=5):\n",
    "    class_weights = torch.tensor([counts[1], counts[0]], dtype=torch.float).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != 'labels'}).logits\n",
    "            loss = loss_fn(logits, batch['labels'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} ì™„ë£Œ\")\n",
    "\n",
    "def evaluate(model, loader, title=\"\"):\n",
    "    print(f\"\\n {title} í‰ê°€ ê²°ê³¼\")\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != 'labels'}).logits\n",
    "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            labs  += batch['labels'].cpu().tolist()\n",
    "    print(classification_report(labs, preds, target_names=['human','ai'], zero_division=0))\n",
    "\n",
    "# 12) ì‹¤í—˜ 1: KG ë¯¸ì‚¬ìš©\n",
    "model = get_model()\n",
    "print(\"\\n[ì‹¤í—˜ 1] KG ë¯¸ì‚¬ìš© í•™ìŠµ ì‹œì‘\")\n",
    "train_model(model, train_loader_no_kg, epochs=5)\n",
    "evaluate(model, test_loader_no_kg, title=\"KG ë¯¸ì‚¬ìš©\")\n",
    "\n",
    "# 13) ì‹¤í—˜ 2: KG ì‚¬ìš©\n",
    "model = get_model()\n",
    "print(\"\\n[ì‹¤í—˜ 2] KG ì‚¬ìš© í•™ìŠµ ì‹œì‘\")\n",
    "train_model(model, train_loader_kg, epochs=5)\n",
    "evaluate(model, test_loader_kg, title=\"KG ì‚¬ìš©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1e64601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KG ë¯¸ì‚¬ìš©] í•™ìŠµ ì‹œì‘\n",
      "Epoch 1 ì™„ë£Œ\n",
      "Epoch 2 ì™„ë£Œ\n",
      "Epoch 3 ì™„ë£Œ\n",
      "Epoch 4 ì™„ë£Œ\n",
      "Epoch 5 ì™„ë£Œ\n",
      "[KG ë¯¸ì‚¬ìš©] í‰ê°€ ê²°ê³¼\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.75      0.08      0.14        38\n",
      "          ai       0.81      0.99      0.89       151\n",
      "\n",
      "    accuracy                           0.81       189\n",
      "   macro avg       0.78      0.54      0.52       189\n",
      "weighted avg       0.80      0.81      0.74       189\n",
      "\n",
      "\n",
      "[KG ì‚¬ìš©] í•™ìŠµ ì‹œì‘\n",
      "Epoch 1 ì™„ë£Œ\n",
      "Epoch 2 ì™„ë£Œ\n",
      "Epoch 3 ì™„ë£Œ\n",
      "Epoch 4 ì™„ë£Œ\n",
      "Epoch 5 ì™„ë£Œ\n",
      "[KG ì‚¬ìš©] í‰ê°€ ê²°ê³¼\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.50      0.11      0.17        38\n",
      "          ai       0.81      0.97      0.89       151\n",
      "\n",
      "    accuracy                           0.80       189\n",
      "   macro avg       0.66      0.54      0.53       189\n",
      "weighted avg       0.75      0.80      0.74       189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ìµœì†Œí™”\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# 1) JSONL íŒŒì¼ ë¡œë“œ\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2) ë¼ë²¨ ì¸ì½”ë”©\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df[\"label\"].dtype == object:\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# 3) KG íŠ¸ë¦¬í”Œ ë¡œë“œ ë° TF-IDF ì¤€ë¹„\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = kg_df.apply(\n",
    "    lambda r: f\"{r['subject']} {r['predicate']} {r['object']}\",\n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "kg_tfidf = tfidf.fit_transform(kg_texts)\n",
    "\n",
    "# 4) TF-IDF ê¸°ë°˜ ìƒìœ„ 3ê°œ KG ê²€ìƒ‰\n",
    "def find_relevant_kg(text, topk=3):\n",
    "    vec = tfidf.transform([text])\n",
    "    sims = cosine_similarity(vec, kg_tfidf)[0]\n",
    "    idxs = sims.argsort()[-topk:][::-1]\n",
    "    return \" \".join(kg_texts[i] for i in idxs)\n",
    "\n",
    "# 5) í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 6) Dataset ì •ì˜ (í•­ìƒ max_lengthë¡œ íŒ¨ë”©/íŠ¸ë í¬)\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        kg   = find_relevant_kg(text) if self.use_kg else None\n",
    "\n",
    "        # â¤ text, kg ë‘˜ ë‹¤ max_length=128ë¡œ íŒ¨ë”©/ìë¥´ê¸°\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            kg,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0).clone() for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.df.loc[idx, \"label\"], dtype=torch.long).clone()\n",
    "        return item\n",
    "\n",
    "# 7) í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# 8) ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 9) ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\n",
    "def get_model():\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"monologg/kobert\", num_labels=2\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# 10) í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜\n",
    "def train_model(model, loader, epochs=5):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.2 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            loss = loss_fn(logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1} ì™„ë£Œ\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            labs  += batch[\"labels\"].cpu().tolist()\n",
    "    print(classification_report(labs, preds, target_names=[\"human\",\"ai\"], zero_division=0))\n",
    "\n",
    "# 11) KG ë¯¸ì‚¬ìš© vs KG ì‚¬ìš© ì‹¤í—˜\n",
    "for use_kg in (False, True):\n",
    "    model = get_model()\n",
    "    train_loader = DataLoader(\n",
    "        PoetryDataset(train_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=True, num_workers=4, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        PoetryDataset(test_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=False, num_workers=4, pin_memory=True\n",
    "    )\n",
    "    mode = \"KG ë¯¸ì‚¬ìš©\" if not use_kg else \"KG ì‚¬ìš©\"\n",
    "    print(f\"\\n[{mode}] í•™ìŠµ ì‹œì‘\")\n",
    "    train_model(model, train_loader, epochs=5)\n",
    "    print(f\"[{mode}] í‰ê°€ ê²°ê³¼\")\n",
    "    evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fc537f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KG ë¯¸ì‚¬ìš©] í•™ìŠµ ì‹œì‘\n",
      "Epoch 1 ì™„ë£Œ\n",
      "Epoch 2 ì™„ë£Œ\n",
      "Epoch 3 ì™„ë£Œ\n",
      "Epoch 4 ì™„ë£Œ\n",
      "Epoch 5 ì™„ë£Œ\n",
      "[KG ë¯¸ì‚¬ìš©] í‰ê°€ ê²°ê³¼\n",
      "              precision    recall  f1-score     support\n",
      "human          0.538462  0.184211  0.274510   38.000000\n",
      "ai             0.823864  0.960265  0.886850  151.000000\n",
      "accuracy       0.804233  0.804233  0.804233    0.804233\n",
      "macro avg      0.681163  0.572238  0.580680  189.000000\n",
      "weighted avg   0.766481  0.804233  0.763734  189.000000\n",
      "\n",
      "[KG ì‚¬ìš©] í•™ìŠµ ì‹œì‘\n",
      "Epoch 1 ì™„ë£Œ\n",
      "Epoch 2 ì™„ë£Œ\n",
      "Epoch 3 ì™„ë£Œ\n",
      "Epoch 4 ì™„ë£Œ\n",
      "Epoch 5 ì™„ë£Œ\n",
      "[KG ì‚¬ìš©] í‰ê°€ ê²°ê³¼\n",
      "              precision    recall  f1-score    support\n",
      "human          0.400000  0.105263  0.166667   38.00000\n",
      "ai             0.810056  0.960265  0.878788  151.00000\n",
      "accuracy       0.788360  0.788360  0.788360    0.78836\n",
      "macro avg      0.605028  0.532764  0.522727  189.00000\n",
      "weighted avg   0.727611  0.788360  0.735610  189.00000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# 1) JSONL ë¡œë“œ í•¨ìˆ˜\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# 2) ë°ì´í„° ì½ê¸° ë° ë¼ë²¨ ì¸ì½”ë”©\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df[\"label\"].dtype == object:\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# 3) KG íŠ¸ë¦¬í”Œ ë¡œë“œ ë° TF-IDF ì¸ë±ìŠ¤ ìƒì„±\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = kg_df.apply(\n",
    "    lambda r: f\"{r['subject']} {r['predicate']} {r['object']}\",\n",
    "    axis=1\n",
    ").tolist()\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "kg_tfidf = tfidf.fit_transform(kg_texts)\n",
    "\n",
    "# 4) TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ KG ê²€ìƒ‰\n",
    "def find_relevant_kg(text, topk=3):\n",
    "    query_vec = tfidf.transform([text])\n",
    "    sims = cosine_similarity(query_vec, kg_tfidf)[0]\n",
    "    idxs = sims.argsort()[-topk:][::-1]\n",
    "    return \" \".join(kg_texts[i] for i in idxs)\n",
    "\n",
    "# 5) KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 6) PyTorch Dataset ì •ì˜\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"text\"]\n",
    "        kg = find_relevant_kg(text) if self.use_kg else None\n",
    "\n",
    "        # ë³¸ë¬¸ê³¼ KGë¥¼ ëª¨ë‘ max_length=128ë¡œ íŒ¨ë”©/íŠ¸ë ì¼€ì´íŠ¸\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            kg,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0).clone() for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(row[\"label\"], dtype=torch.long).clone()\n",
    "        return item\n",
    "\n",
    "# 7) í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# 8) ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 9) ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\n",
    "def get_model():\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"monologg/kobert\",\n",
    "        num_labels=2\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# 10) í•™ìŠµ í•¨ìˆ˜\n",
    "def train_model(model, loader, epochs=5):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.2 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            loss = loss_fn(logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1} ì™„ë£Œ\")\n",
    "\n",
    "# 11) í‰ê°€ í•¨ìˆ˜ (ì „ì²´ ë¦¬í¬íŠ¸ ë³´ì—¬ì£¼ê¸°)\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            labs  += batch[\"labels\"].cpu().tolist()\n",
    "\n",
    "    # pandas ì¶œë ¥ ì˜µì…˜: ëª¨ë“  ì—´ê³¼ ë„“ì´ë¥¼ ë³´ì—¬ ì¤Œ\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "    # classification_reportë¥¼ dictë¡œ ë°›ì•„ DataFrameìœ¼ë¡œ ì¶œë ¥\n",
    "    report_dict = classification_report(labs, preds, target_names=[\"human\",\"ai\"],\n",
    "                                        output_dict=True, digits=4)\n",
    "    df_report = pd.DataFrame(report_dict).T\n",
    "    print(df_report)\n",
    "\n",
    "# 12) KG ë¯¸ì‚¬ìš© vs KG ì‚¬ìš© ì‹¤í—˜\n",
    "for use_kg in (False, True):\n",
    "    model = get_model()\n",
    "    train_loader = DataLoader(\n",
    "        PoetryDataset(train_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=True,\n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        PoetryDataset(test_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=False,\n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    mode = \"KG ë¯¸ì‚¬ìš©\" if not use_kg else \"KG ì‚¬ìš©\"\n",
    "    print(f\"\\n[{mode}] í•™ìŠµ ì‹œì‘\")\n",
    "    train_model(model, train_loader, epochs=5)\n",
    "    print(f\"[{mode}] í‰ê°€ ê²°ê³¼\")\n",
    "    evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b1cd228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KG ë¯¸ì‚¬ìš©] í•™ìŠµ ì‹œì‘\n",
      "Epoch 1 ì™„ë£Œ\n",
      "Epoch 2 ì™„ë£Œ\n",
      "Epoch 3 ì™„ë£Œ\n",
      "Epoch 4 ì™„ë£Œ\n",
      "Epoch 5 ì™„ë£Œ\n",
      "[KG ë¯¸ì‚¬ìš©] í‰ê°€ ê²°ê³¼\n",
      "              precision    recall  f1-score     support\n",
      "human          0.336634  0.894737  0.489209   38.000000\n",
      "ai             0.954545  0.556291  0.702929  151.000000\n",
      "accuracy       0.624339  0.624339  0.624339    0.624339\n",
      "macro avg      0.645590  0.725514  0.596069  189.000000\n",
      "weighted avg   0.830309  0.624339  0.659959  189.000000\n",
      "\n",
      "[KG ì‚¬ìš©] í•™ìŠµ ì‹œì‘\n",
      "Epoch 1 ì™„ë£Œ\n",
      "Epoch 2 ì™„ë£Œ\n",
      "Epoch 3 ì™„ë£Œ\n",
      "Epoch 4 ì™„ë£Œ\n",
      "Epoch 5 ì™„ë£Œ\n",
      "[KG ì‚¬ìš©] í‰ê°€ ê²°ê³¼\n",
      "              precision    recall  f1-score     support\n",
      "human          0.282443  0.973684  0.437870   38.000000\n",
      "ai             0.982759  0.377483  0.545455  151.000000\n",
      "accuracy       0.497354  0.497354  0.497354    0.497354\n",
      "macro avg      0.632601  0.675584  0.491662  189.000000\n",
      "weighted avg   0.841954  0.497354  0.523824  189.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import json\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ìµœì†Œí™”\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# 1) JSONL íŒŒì¼ ë¡œë“œ\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2) ë¼ë²¨ ì¸ì½”ë”©\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df[\"label\"].dtype == object:\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# 3) KG íŠ¸ë¦¬í”Œ ë¡œë“œ ë° TF-IDF ì¸ë±ìŠ¤ ìƒì„±\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = kg_df.apply(\n",
    "    lambda r: f\"{r['subject']} {r['predicate']} {r['object']}\",\n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "kg_tfidf = tfidf.fit_transform(kg_texts)\n",
    "\n",
    "# 4) TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ top3 KG ê²€ìƒ‰\n",
    "def find_relevant_kg(text, topk=3):\n",
    "    qv = tfidf.transform([text])\n",
    "    sims = cosine_similarity(qv, kg_tfidf)[0]\n",
    "    idxs = sims.argsort()[-topk:][::-1]\n",
    "    return \" \".join(kg_texts[i] for i in idxs)\n",
    "\n",
    "# 5) KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 6) Dataset ì •ì˜ (padding+truncation í†µì¼)\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"text\"]\n",
    "        kg   = find_relevant_kg(text) if self.use_kg else None\n",
    "\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            kg,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0).clone() for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(row[\"label\"], dtype=torch.long).clone()\n",
    "        return item\n",
    "\n",
    "# 7) í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# 8) oversampling ìœ„í•œ WeightedRandomSampler ìƒì„±\n",
    "counts = train_df[\"label\"].value_counts().sort_index().tolist()  # [human_count, ai_count]\n",
    "# ê° ìƒ˜í”Œì— inverse frequency ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
    "sample_weights = [1.0 / counts[label] for label in train_df[\"label\"]]\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# 9) ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 10) ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\n",
    "def get_model():\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"monologg/kobert\", num_labels=2\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# 11) í•™ìŠµ í•¨ìˆ˜ (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ + ìŠ¤ì¼€ì¤„ëŸ¬)\n",
    "def train_model(model, loader, epochs=5):\n",
    "    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ëŠ” [ai_count, human_count] ìˆœì´ ë˜ë„ë¡\n",
    "    class_weights = torch.tensor([counts[1], counts[0]], dtype=torch.float).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.2 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            loss = loss_fn(logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch} ì™„ë£Œ\")\n",
    "\n",
    "# 12) í‰ê°€ í•¨ìˆ˜ (ì „ì²´ ë¦¬í¬íŠ¸ ì¶œë ¥)\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            labs  += batch[\"labels\"].cpu().tolist()\n",
    "\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.width\", 1000)\n",
    "    report = classification_report(\n",
    "        labs, preds,\n",
    "        target_names=[\"human\",\"ai\"],\n",
    "        output_dict=True,\n",
    "        digits=4\n",
    "    )\n",
    "    print(pd.DataFrame(report).T)\n",
    "\n",
    "# 13) KG ë¯¸ì‚¬ìš© vs KG ì‚¬ìš© ì‹¤í—˜\n",
    "for use_kg in (False, True):\n",
    "    model = get_model()\n",
    "    train_loader = DataLoader(\n",
    "        PoetryDataset(train_df, use_kg=use_kg),\n",
    "        batch_size=16,\n",
    "        sampler=sampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        PoetryDataset(test_df, use_kg=use_kg),\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    mode = \"KG ë¯¸ì‚¬ìš©\" if not use_kg else \"KG ì‚¬ìš©\"\n",
    "    print(f\"\\n[{mode}] í•™ìŠµ ì‹œì‘\")\n",
    "    train_model(model, train_loader, epochs=5)\n",
    "    print(f\"[{mode}] í‰ê°€ ê²°ê³¼\")\n",
    "    evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7957ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KG ì—†ì´] ì‹¤í—˜ ì‹œì‘\n",
      "Epoch 1 ì™„ë£Œ\n",
      "Epoch 2 ì™„ë£Œ\n",
      "Epoch 3 ì™„ë£Œ\n",
      "Epoch 4 ì™„ë£Œ\n",
      "Epoch 5 ì™„ë£Œ\n",
      "[KG ì—†ì´] í‰ê°€ ê²°ê³¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ficl/cuda-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ficl/cuda-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ficl/cuda-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human     0.0000    0.0000    0.0000        38\n",
      "          ai     0.7989    1.0000    0.8882       151\n",
      "\n",
      "    accuracy                         0.7989       189\n",
      "   macro avg     0.3995    0.5000    0.4441       189\n",
      "weighted avg     0.6383    0.7989    0.7096       189\n",
      "\n",
      "\n",
      "[KG í¬í•¨] ì‹¤í—˜ ì‹œì‘\n",
      "Epoch 1 ì™„ë£Œ\n",
      "Epoch 2 ì™„ë£Œ\n",
      "Epoch 3 ì™„ë£Œ\n",
      "Epoch 4 ì™„ë£Œ\n",
      "Epoch 5 ì™„ë£Œ\n",
      "[KG í¬í•¨] í‰ê°€ ê²°ê³¼\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human     0.5000    0.0263    0.0500        38\n",
      "          ai     0.8021    0.9934    0.8876       151\n",
      "\n",
      "    accuracy                         0.7989       189\n",
      "   macro avg     0.6511    0.5098    0.4688       189\n",
      "weighted avg     0.7414    0.7989    0.7192       189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ ìµœì†Œí™”\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# 1) ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df[\"label\"].dtype == object:\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# 2) KG íŠ¸ë¦¬í”Œ ë¡œë“œ ë° TF-IDF ì¤€ë¹„\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = kg_df.apply(\n",
    "    lambda r: f\"{r['subject']} {r['predicate']} {r['object']}\",\n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf.fit(kg_texts)\n",
    "\n",
    "# 3) KG ê²€ìƒ‰ í•¨ìˆ˜ ( ì½”ì‚¬ì¸ ìœ ì‚¬ë„ )\n",
    "def find_relevant_kg(text, topk=3):\n",
    "    qv = tfidf.transform([text])\n",
    "    sims = cosine_similarity(qv, tfidf.transform(kg_texts))[0]\n",
    "    idxs = sims.argsort()[-topk:][::-1]\n",
    "    return \" \".join(kg_texts[i] for i in idxs)\n",
    "\n",
    "# 4) Dataset ì •ì˜ (late-fusion)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "tfidf_dim = len(tfidf.vocabulary_)\n",
    "\n",
    "class LateFusionDataset(Dataset): # ëª¨ë¸ KoBERT\n",
    "    def __init__(self, df, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"text\"]\n",
    "        kg_text = find_relevant_kg(text) if self.use_kg else \"\"\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "        kg_vec = torch.tensor(tfidf.transform([kg_text]).toarray()[0], dtype=torch.float)\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "        return input_ids, attention_mask, kg_vec, label\n",
    "\n",
    "# 5) Late-fusion ëª¨ë¸ ì •ì˜\n",
    "class LateFusionModel(nn.Module):\n",
    "    def __init__(self, tfidf_dim):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.kg_mlp = nn.Sequential(\n",
    "            nn.Linear(tfidf_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, kg_vec):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_pool = out.pooler_output                 # (batch, hidden)\n",
    "        kg_emb = self.kg_mlp(kg_vec)                  # (batch, hidden)\n",
    "        fusion = torch.cat([text_pool, kg_emb], dim=1)  # (batch, hidden*2)\n",
    "        logits = self.classifier(fusion)              # (batch, 2)\n",
    "        return logits\n",
    "\n",
    "# 6) í•™ìŠµÂ·í‰ê°€ í•¨ìˆ˜\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model, loader, epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        for input_ids, attention_mask, kg_vec, labels in loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            kg_vec = kg_vec.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(input_ids, attention_mask, kg_vec)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {ep} ì™„ë£Œ\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, kg_vec, labels in loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            kg_vec = kg_vec.to(device)\n",
    "            logits = model(input_ids, attention_mask, kg_vec)\n",
    "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            labs  += labels.tolist()\n",
    "    print(classification_report(labs, preds, target_names=[\"human\",\"ai\"], digits=4))\n",
    "\n",
    "# 7) ë°ì´í„°ë¡œë” ë° ì‹¤í—˜ ì‹¤í–‰\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "for use_kg in (False, True):\n",
    "    mode = \"KG ì—†ì´\" if not use_kg else \"KG í¬í•¨\"\n",
    "    print(f\"\\n[{mode}] ì‹¤í—˜ ì‹œì‘\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        LateFusionDataset(train_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=True, num_workers=4, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        LateFusionDataset(test_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=False, num_workers=4, pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = LateFusionModel(tfidf_dim)\n",
    "    train(model, train_loader, epochs=5)\n",
    "    print(f\"[{mode}] í‰ê°€ ê²°ê³¼\")\n",
    "    evaluate(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
