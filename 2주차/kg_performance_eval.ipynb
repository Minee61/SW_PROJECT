{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8e31f4",
   "metadata": {},
   "source": [
    "# KG ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜ (ROUGE ê¸°ë°˜ í‰ê°€)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d3a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "kg = pd.read_csv(\"kg_triples_test.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# íŠ¸ë¦¬í”Œ ê²°í•© í…ìŠ¤íŠ¸ ë§Œë“¤ê¸°\n",
    "kg[\"triple_text\"] = kg[\"subject\"] + \" \" + kg[\"predicate\"] + \" \" + kg[\"object\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac6233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê¸°ë°˜ KG ê²€ìƒ‰ í•¨ìˆ˜\n",
    "def find_relevant_kg(text, kg_texts, top_k=3):\n",
    "    matches = []\n",
    "    for t in kg_texts:\n",
    "        score = sum([1 for w in t.split() if w in text])\n",
    "        matches.append((t, score))\n",
    "    matches.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [x[0] for x in matches[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55bbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ êµ¬ì„± í•¨ìˆ˜\n",
    "def build_prompt(text, kg_hits):\n",
    "    context = \"\\n\".join(kg_hits)\n",
    "    return f\"ë‹¤ìŒì€ ì°¸ê³  ì§€ì‹ì…ë‹ˆë‹¤:\\n{context}\\n\\nì‚¬ìš©ì ì…ë ¥: {text}\\në‹µë³€:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d974c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE í‰ê°€ í•¨ìˆ˜\n",
    "def evaluate_with_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, pred)[\"rougeL\"].fmeasure for pred, ref in zip(predictions, references)]\n",
    "    return sum(scores) / len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a94795",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'input'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m sample = test_df.sample(\u001b[32m10\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1. KG ì—†ì´ (baseline)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m preds_no_kg = [\u001b[33m\"\u001b[39m\u001b[33më‹µë³€: \u001b[39m\u001b[33m\"\u001b[39m + text[:\u001b[32m20\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. KG ì‚¬ìš©\u001b[39;00m\n\u001b[32m     10\u001b[39m preds_with_kg = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'input'"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# ìƒ˜í”Œ 10ê°œ ì¶”ì¶œ\n",
    "sample = test_df.sample(10, random_state=42)\n",
    "\n",
    "# 1. KG ì—†ì´ (baseline)\n",
    "preds_no_kg = [\"ë‹µë³€: \" + text[:20] for text in sample[\"input\"]]\n",
    "\n",
    "# 2. KG ì‚¬ìš©\n",
    "preds_with_kg = []\n",
    "for text in sample[\"input\"]:\n",
    "    hits = find_relevant_kg(text, kg[\"triple_text\"])\n",
    "    prompt = build_prompt(text, hits)\n",
    "    preds_with_kg.append(\"ë‹µë³€: \" + text[:20] + \" (ì§€ì‹ë°˜ì˜)\")\n",
    "\n",
    "# í‰ê°€\n",
    "rouge_no_kg = evaluate_with_rouge(preds_no_kg, sample[\"output\"])\n",
    "rouge_with_kg = evaluate_with_rouge(preds_with_kg, sample[\"output\"])\n",
    "\n",
    "print(f\"KG ë¯¸ì‚¬ìš© ì‹œ ROUGE-L: {rouge_no_kg:.4f}\")\n",
    "print(f\"KG ì‚¬ìš© ì‹œ ROUGE-L: {rouge_with_kg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22f11a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì§ˆë¬¸: ë¼ì´ìŠ¤ í˜ì´í¼ì˜ ë‘ê»˜ë‚˜ êµ¬ì„±ì´ ì¼ë°˜ì ì´ì§€ ì•Šì•„ ì¡°ê¸ˆ ë†€ëìŠµë‹ˆë‹¤. ì•„ë§ˆ, ì‹ì‚¬ëŸ‰ì„ ì¶©ë¶„íˆ í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜€ì„ê±´ë°, ì´ë¡œ ì¸í•´ ì›”ë‚¨ìŒˆì˜ ì§ˆê°ì´ë‚˜ ë§›ì„ ëŠë¼ëŠ” ë¶€ë¶„ì—ì„œ ì¡°ê¸ˆ í˜ë“¤ì—ˆìŠµë‹ˆë‹¤. ê°œì¸ì ìœ¼ë¡œëŠ” í› ê¶ˆ ìœ¡ìˆ˜ëŠ” ì¡°ê¸ˆ ì•„ì‰¬ì›€ì´ ë‚¨ì•˜ì§€ë§Œ, ë‹¤ë¥¸ ë©”ë‰´ë“¤ ì¤‘ì—ì„œëŠ” ê³ ê¸°ê°€ ë¹¼ì–´ë‚œ ë§›ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n",
      "\n",
      "[í”„ë¡¬í”„íŠ¸ - KG ë¯¸ì‚¬ìš©]\n",
      " ì§ˆë¬¸: ë¼ì´ìŠ¤ í˜ì´í¼ì˜ ë‘ê»˜ë‚˜ êµ¬ì„±ì´ ì¼ë°˜ì ì´ì§€ ì•Šì•„ ì¡°ê¸ˆ ë†€ëìŠµë‹ˆë‹¤. ì•„ë§ˆ, ì‹ì‚¬ëŸ‰ì„ ì¶©ë¶„íˆ í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜€ì„ê±´ë°, ì´ë¡œ ì¸í•´ ì›”ë‚¨ìŒˆì˜ ì§ˆê°ì´ë‚˜ ë§›ì„ ëŠë¼ëŠ” ë¶€ë¶„ì—ì„œ ì¡°ê¸ˆ í˜ë“¤ì—ˆìŠµë‹ˆë‹¤. ê°œì¸ì ìœ¼ë¡œëŠ” í› ê¶ˆ ìœ¡ìˆ˜ëŠ” ì¡°ê¸ˆ ì•„ì‰¬ì›€ì´ ë‚¨ì•˜ì§€ë§Œ, ë‹¤ë¥¸ ë©”ë‰´ë“¤ ì¤‘ì—ì„œëŠ” ê³ ê¸°ê°€ ë¹¼ì–´ë‚œ ë§›ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n",
      "ë‹µë³€:\n",
      "\n",
      "[í”„ë¡¬í”„íŠ¸ - KG ì‚¬ìš©]\n",
      " ì§ˆë¬¸: ë¼ì´ìŠ¤ í˜ì´í¼ì˜ ë‘ê»˜ë‚˜ êµ¬ì„±ì´ ì¼ë°˜ì ì´ì§€ ì•Šì•„ ì¡°ê¸ˆ ë†€ëìŠµë‹ˆë‹¤. ì•„ë§ˆ, ì‹ì‚¬ëŸ‰ì„ ì¶©ë¶„íˆ í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜€ì„ê±´ë°, ì´ë¡œ ì¸í•´ ì›”ë‚¨ìŒˆì˜ ì§ˆê°ì´ë‚˜ ë§›ì„ ëŠë¼ëŠ” ë¶€ë¶„ì—ì„œ ì¡°ê¸ˆ í˜ë“¤ì—ˆìŠµë‹ˆë‹¤. ê°œì¸ì ìœ¼ë¡œëŠ” í› ê¶ˆ ìœ¡ìˆ˜ëŠ” ì¡°ê¸ˆ ì•„ì‰¬ì›€ì´ ë‚¨ì•˜ì§€ë§Œ, ë‹¤ë¥¸ ë©”ë‰´ë“¤ ì¤‘ì—ì„œëŠ” ê³ ê¸°ê°€ ë¹¼ì–´ë‚œ ë§›ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n",
      "ë°°ê²½ì§€ì‹:\n",
      "- ì‚¬ìš©ì ì§ˆë¬¸í•œë‹¤ ë‹¤ë¥¸ ì‚¬ëŒì˜ ì˜ë„\n",
      "ë‹µë³€:\n",
      "\n",
      "ğŸ¤– [KG ë¯¸ì‚¬ìš© ë‹µë³€]\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] ì§€ì •ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# âœ… 8. Ollama ì‹¤í–‰ ë° ì¶œë ¥ ê²°ê³¼\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ¤– [KG ë¯¸ì‚¬ìš© ë‹µë³€]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrun_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_no_kg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ¤– [KG ì‚¬ìš© ë‹µë³€]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(run_ollama(prompt_kg))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mrun_ollama\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_ollama\u001b[39m(prompt, model=\u001b[33m\"\u001b[39m\u001b[33mllama3\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     result = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mollama\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.stdout.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\subprocess.py:556\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstdout\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m    554\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstderr\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    558\u001b[39m         stdout, stderr = process.communicate(\u001b[38;5;28minput\u001b[39m, timeout=timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\subprocess.py:1038\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1034\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1035\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1036\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1048\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\subprocess.py:1550\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[39m\n\u001b[32m   1548\u001b[39m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     hp, ht, pid, tid = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[32m   1552\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1554\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1559\u001b[39m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[32m   1560\u001b[39m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1563\u001b[39m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[32m   1564\u001b[39m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[32m   1565\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_pipe_fds(p2cread, p2cwrite,\n\u001b[32m   1566\u001b[39m                          c2pread, c2pwrite,\n\u001b[32m   1567\u001b[39m                          errread, errwrite)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] ì§€ì •ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
     ]
    }
   ],
   "source": [
    "# âœ… 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”©\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "# âœ… 2. Knowledge Graph ë¡œë”© ë° í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "kg = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg[\"triple_text\"] = kg[\"subject\"] + \" \" + kg[\"predicate\"] + \" \" + kg[\"object\"]\n",
    "\n",
    "# âœ… 3. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ë¡œë”©\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# âœ… 4. KG ê²€ìƒ‰ í•¨ìˆ˜\n",
    "def find_relevant_kg(user_input, kg_texts, topk=3):\n",
    "    return [t for t in kg_texts if any(word in user_input for word in t.split())][:topk]\n",
    "\n",
    "# âœ… 5. í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜\n",
    "def build_prompt(user_input, kg_hits=None):\n",
    "    prompt = f\"ì§ˆë¬¸: {user_input}\\n\"\n",
    "    if kg_hits:\n",
    "        prompt += \"ë°°ê²½ì§€ì‹:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += \"ë‹µë³€:\"\n",
    "    return prompt\n",
    "\n",
    "# âœ… 6. ë¡œì»¬ Ollama ëª¨ë¸ í˜¸ì¶œ í•¨ìˆ˜ (ì˜ˆ: llama3)\n",
    "def run_ollama(prompt, model=\"llama3\"):\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt.encode(\"utf-8\"),\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        timeout=30\n",
    "    )\n",
    "    return result.stdout.decode(\"utf-8\").strip()\n",
    "\n",
    "# âœ… 7. ì§ˆë¬¸ ì…ë ¥ â†’ KG ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ë¹„êµ\n",
    "sample = test_df.sample(1, random_state=42)\n",
    "user_input = sample[\"sentence1\"].values[0]\n",
    "true_answer = sample[\"sentence2\"].values[0]\n",
    "\n",
    "kg_hits = find_relevant_kg(user_input, kg[\"triple_text\"])\n",
    "prompt_kg = build_prompt(user_input, kg_hits)\n",
    "prompt_no_kg = build_prompt(user_input)\n",
    "\n",
    "print(\"ğŸ“Œ ì§ˆë¬¸:\", user_input)\n",
    "print(\"\\n[í”„ë¡¬í”„íŠ¸ - KG ë¯¸ì‚¬ìš©]\\n\", prompt_no_kg)\n",
    "print(\"\\n[í”„ë¡¬í”„íŠ¸ - KG ì‚¬ìš©]\\n\", prompt_kg)\n",
    "\n",
    "# âœ… 8. Ollama ì‹¤í–‰ ë° ì¶œë ¥ ê²°ê³¼\n",
    "print(\"\\nğŸ¤– [KG ë¯¸ì‚¬ìš© ë‹µë³€]\\n\")\n",
    "print(run_ollama(prompt_no_kg))\n",
    "\n",
    "print(\"\\nğŸ¤– [KG ì‚¬ìš© ë‹µë³€]\\n\")\n",
    "print(run_ollama(prompt_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53429b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ KG ë¯¸ì‚¬ìš©:\n",
      "í”„ë¡¬í”„íŠ¸: ì§ˆë¬¸: ì§ˆë¬¸: ë‚˜ëŠ” [MASK] ê¸°ë¶„ì´ë‹¤\n",
      "ë‹µë³€: ì§ˆë¬¸: ì§ˆë¬¸: ë‚˜ëŠ” ê½‰ ê¸°ë¶„ì´ë‹¤\n",
      "\n",
      "ğŸ“Œ KG ì‚¬ìš©:\n",
      "í”„ë¡¬í”„íŠ¸: ì§ˆë¬¸: ì§ˆë¬¸: ë‚˜ëŠ” [MASK] ê¸°ë¶„ì´ë‹¤\n",
      "ë‹µë³€: ì§ˆë¬¸: ì§ˆë¬¸: ë‚˜ëŠ” ê½‰ ê¸°ë¶„ì´ë‹¤\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import pandas as pd\n",
    "\n",
    "# CPUë¡œ ê°•ì œ ì„¤ì •\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# KoBERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertForMaskedLM.from_pretrained('monologg/kobert')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# KG ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "if 'triple_text' not in kg_df.columns:\n",
    "    kg_df['triple_text'] = kg_df['subject'] + \" \" + kg_df['predicate'] + \" \" + kg_df['object']\n",
    "\n",
    "# ë°°ê²½ì§€ì‹ ì°¾ê¸°\n",
    "def find_relevant_kg(question, kg_df):\n",
    "    hits = []\n",
    "    for triple in kg_df['triple_text']:\n",
    "        if any(word in triple for word in question.split()):\n",
    "            hits.append(triple)\n",
    "    return hits[:3]\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "def build_prompt(question, kg_hits=None):\n",
    "    prompt = \"\"\n",
    "    if kg_hits:\n",
    "        prompt += \"ë°°ê²½ì§€ì‹:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += f\"ì§ˆë¬¸: {question}\"\n",
    "    return prompt\n",
    "\n",
    "# ë‹µë³€ ìƒì„±\n",
    "def generate_answer(prompt):\n",
    "    if \"[MASK]\" not in prompt:\n",
    "        return \"ì§ˆë¬¸ì— [MASK]ê°€ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = outputs.logits[0, mask_token_index, :]\n",
    "    top_token = torch.argmax(mask_token_logits, dim=1)\n",
    "    predicted_token = tokenizer.decode(top_token)\n",
    "    \n",
    "    return prompt.replace(\"[MASK]\", predicted_token)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "user_question = input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë‚˜ëŠ” [MASK] ê¸°ë¶„ì´ë‹¤): \")\n",
    "\n",
    "# KG ë¯¸ì‚¬ìš©\n",
    "prompt_no_kg = build_prompt(user_question)\n",
    "answer_no_kg = generate_answer(prompt_no_kg)\n",
    "\n",
    "# KG ì‚¬ìš©\n",
    "kg_hits = find_relevant_kg(user_question, kg_df)\n",
    "prompt_with_kg = build_prompt(user_question, kg_hits)\n",
    "answer_with_kg = generate_answer(prompt_with_kg)\n",
    "\n",
    "print(\"\\n KG ë¯¸ì‚¬ìš©:\")\n",
    "print(\"í”„ë¡¬í”„íŠ¸:\", prompt_no_kg)\n",
    "print(\"ë‹µë³€:\", answer_no_kg)\n",
    "\n",
    "print(\"\\n KG ì‚¬ìš©:\")\n",
    "print(\"í”„ë¡¬í”„íŠ¸:\", prompt_with_kg)\n",
    "print(\"ë‹µë³€:\", answer_with_kg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ [KG ë¯¸ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\n",
      " ì§ˆë¬¸: í”¼ê³¤í•´\n",
      "ë‹µë³€:\n",
      "\n",
      "ğŸ¤– [KG ë¯¸ì‚¬ìš© ë‹µë³€]\n",
      " â±ï¸ ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Œ [KG ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\n",
      " ì§ˆë¬¸: í”¼ê³¤í•´\n",
      "ë‹µë³€:\n",
      "\n",
      "ğŸ¤– [KG ì‚¬ìš© ë‹µë³€]\n",
      " â±ï¸ ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "# Ollama ì‹¤í–‰ ê²½ë¡œ (ì§ì ‘ ì§€ì •)\n",
    "OLLAMA_PATH = r\"C:\\Users\\lsm40\\AppData\\Local\\Programs\\Ollama\\ollama.exe\"\n",
    "\n",
    "# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥\n",
    "user_input = input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "# KG íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")  # ê²½ë¡œëŠ” í•„ìš”ì— ë”°ë¼ ìˆ˜ì •\n",
    "\n",
    "# KGì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰)\n",
    "def find_relevant_kg(question, kg_df):\n",
    "    hits = []\n",
    "    for idx, row in kg_df.iterrows():\n",
    "        triple = f\"{row['subject']}, {row['predicate']}, {row['object']}\"\n",
    "        if any(word in triple for word in question.split()):\n",
    "            hits.append(triple)\n",
    "    return hits[:3]  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì‚¬ìš©\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "def build_prompt(user_input, kg_hits=None):\n",
    "    prompt = \"\"\n",
    "    if kg_hits:\n",
    "        prompt += \"ë°°ê²½ì§€ì‹:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += f\"ì§ˆë¬¸: {user_input}\\në‹µë³€:\"\n",
    "    return prompt\n",
    "\n",
    "#  Llama3ë¡œ ë‹µë³€ ìƒì„±\n",
    "def run_ollama(prompt, model=\"llama3\"):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [OLLAMA_PATH, \"run\", model],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=60\n",
    "        )\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except FileNotFoundError:\n",
    "        return \" Ollama ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \" ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "    except Exception as e:\n",
    "        return f\" ì˜¤ë¥˜ ë°œìƒ: {e}\"\n",
    "\n",
    "#  KG ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "kg_hits = find_relevant_kg(user_input, kg_df)\n",
    "\n",
    "#  í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "prompt_kg = build_prompt(user_input, kg_hits)\n",
    "prompt_no_kg = build_prompt(user_input)\n",
    "\n",
    "#  ì¶œë ¥\n",
    "print(\"\\n [KG ë¯¸ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\\n\", prompt_no_kg)\n",
    "print(\"\\n [KG ë¯¸ì‚¬ìš© ë‹µë³€]\\n\", run_ollama(prompt_no_kg))\n",
    "\n",
    "print(\"\\n [KG ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\\n\", prompt_kg)\n",
    "print(\"\\n [KG ì‚¬ìš© ë‹µë³€]\\n\", run_ollama(prompt_kg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51647132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ [KG ë¯¸ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\n",
      " ì§ˆë¬¸: ë‚˜ í”¼ê³¤í•´\n",
      "ë‹µë³€:\n",
      "\n",
      "ğŸ¤– [KG ë¯¸ì‚¬ìš© ë‹µë³€]\n",
      " â±ï¸ ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ“Œ [KG ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\n",
      " ì§ˆë¬¸: ë‚˜ í”¼ê³¤í•´\n",
      "ë‹µë³€:\n",
      "\n",
      "ğŸ¤– [KG ì‚¬ìš© ë‹µë³€]\n",
      " â±ï¸ ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#  Ollama ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ (ì§ì ‘ í™•ì¸ëœ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)\n",
    "OLLAMA_PATH = r\"C:\\Users\\lsm40\\AppData\\Local\\Programs\\Ollama\\ollama.exe\"\n",
    "\n",
    "#  ì‚¬ìš©ì ì§ˆë¬¸ ì‹¤ì‹œê°„ ì…ë ¥\n",
    "user_input = input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "\n",
    "#  KG ê·¸ë˜í”„ CSV ë¡œë”© (ì»¬ëŸ¼ëª… ìë™ ê°ì§€)\n",
    "kg_path = \"./kg_triples_test.csv\"  # ì˜ˆ: 'triple_text' ì—´ì´ ìˆëŠ” íŒŒì¼\n",
    "kg_df = pd.read_csv(kg_path)\n",
    "col = kg_df.columns[0]  # ì²« ë²ˆì§¸ ì—´ ìë™ ì¸ì‹\n",
    "\n",
    "#  KGì—ì„œ ê´€ë ¨ëœ ë°°ê²½ì§€ì‹ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def find_relevant_kg(question, df, column):\n",
    "    hits = []\n",
    "    for triple in df[column]:\n",
    "        if any(word in triple for word in question.split()):\n",
    "            hits.append(triple)\n",
    "    return hits[:3]\n",
    "\n",
    "#  í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜\n",
    "def build_prompt(question, kg_hits=None):\n",
    "    prompt = \"\"\n",
    "    if kg_hits:\n",
    "        prompt += \"ë°°ê²½ì§€ì‹:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += f\"ì§ˆë¬¸: {question}\\në‹µë³€:\"\n",
    "    return prompt\n",
    "\n",
    "#  Llama3 ì‹¤í–‰ í•¨ìˆ˜\n",
    "def run_ollama(prompt, timeout=60):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [OLLAMA_PATH, \"run\", \"llama3\"],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        if result.stderr:\n",
    "            print(\" stderr:\", result.stderr.decode(\"utf-8\").strip())\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \" ì‹¤í–‰ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "    except FileNotFoundError:\n",
    "        return \" Ollama ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\"\n",
    "\n",
    "#  KG ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "kg_hits = find_relevant_kg(user_input, kg_df, col)\n",
    "prompt_no_kg = build_prompt(user_input)\n",
    "prompt_kg = build_prompt(user_input, kg_hits)\n",
    "\n",
    "#  ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n [KG ë¯¸ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\\n\", prompt_no_kg)\n",
    "print(\"\\n [KG ë¯¸ì‚¬ìš© ë‹µë³€]\\n\", run_ollama(prompt_no_kg))\n",
    "\n",
    "print(\"\\n [KG ì‚¬ìš© í”„ë¡¬í”„íŠ¸]\\n\", prompt_kg)\n",
    "print(\"\\n [KG ì‚¬ìš© ë‹µë³€]\\n\", run_ollama(prompt_kg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e98a60b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m item\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# âœ… ë°ì´í„° ë¶„ë¦¬ (8:2), stratify ê°€ëŠ¥\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m train_df, test_df = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     76\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# âœ… ëª¨ë¸ ì •ì˜\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_model\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2851\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2848\u001b[39m arrays = indexable(*arrays)\n\u001b[32m   2850\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2851\u001b[39m n_train, n_test = \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\n\u001b[32m   2853\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   2856\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2481\u001b[39m, in \u001b[36m_validate_shuffle_split\u001b[39m\u001b[34m(n_samples, test_size, train_size, default_test_size)\u001b[39m\n\u001b[32m   2478\u001b[39m n_train, n_test = \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[32m   2480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2481\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2482\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2483\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maforementioned parameters.\u001b[39m\u001b[33m\"\u001b[39m.format(n_samples, test_size, train_size)\n\u001b[32m   2485\u001b[39m     )\n\u001b[32m   2487\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[31mValueError\u001b[39m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# âœ… KoBERT ë¡œë“œ\n",
    "model_name = 'monologg/kobert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# âœ… ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# âœ… ë¼ë²¨ ì¸ì½”ë”©: \"human\" â†’ 0, \"ai\" â†’ 1\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "df['label'] = df['label'].map(label_map)\n",
    "\n",
    "# âœ… NaN ì œê±° (ë§¤í•‘ë˜ì§€ ì•Šì€ ë¼ë²¨ ìˆëŠ” í–‰ ì œê±°)\n",
    "df = df.dropna(subset=['label'])\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# âœ… KG ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = (\n",
    "    kg_df\n",
    "    .apply(lambda row: f\"{row['subject']} {row['predicate']} {row['object']}\", axis=1)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# âœ… KG ê´€ë ¨ ë°°ê²½ì§€ì‹ ì°¾ê¸° (ê°„ë‹¨ í‚¤ì›Œë“œ ë§¤ì¹­)\n",
    "def find_relevant_kg(text, kg_texts, topk=3):\n",
    "    hits = [kg for kg in kg_texts if any(word in kg for word in text.split())]\n",
    "    return \" \".join(hits[:topk])\n",
    "\n",
    "# âœ… PyTorch Dataset\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, dataframe, use_kg=False):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = row['text']\n",
    "        if self.use_kg:\n",
    "            kg = find_relevant_kg(text, kg_texts)\n",
    "            text = f\"{text} [SEP] {kg}\"\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        item[\"labels\"] = torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# âœ… ë°ì´í„° ë¶„ë¦¬ (8:2), stratify ê°€ëŠ¥\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# âœ… ëª¨ë¸ ì •ì˜\n",
    "def get_model():\n",
    "    return BertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "# âœ… í•™ìŠµ í•¨ìˆ˜\n",
    "def train_model(model, train_loader, epochs=3):\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1} ì™„ë£Œ\")\n",
    "\n",
    "# âœ… í‰ê°€ í•¨ìˆ˜\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds += torch.argmax(logits, axis=1).cpu().tolist()\n",
    "            labels += batch[\"labels\"].cpu().tolist()\n",
    "    print(classification_report(labels, preds, target_names=[\"human\", \"ai\"]))\n",
    "\n",
    "\n",
    "# ---- KG ë¯¸ì‚¬ìš© ì‹¤í—˜ ----\n",
    "model = get_model()\n",
    "train_loader_no_kg = DataLoader(\n",
    "    PoetryDataset(train_df, use_kg=False),\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader_no_kg = DataLoader(\n",
    "    PoetryDataset(test_df, use_kg=False),\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(\"ğŸ”¥ KG ë¯¸ì‚¬ìš© í•™ìŠµ ì‹œì‘\")\n",
    "train_model(model, train_loader_no_kg)\n",
    "print(\"ğŸ“Š KG ë¯¸ì‚¬ìš© ì„±ëŠ¥\")\n",
    "evaluate(model, test_loader_no_kg)\n",
    "\n",
    "\n",
    "# ---- KG ì‚¬ìš© ì‹¤í—˜ ----\n",
    "model = get_model()\n",
    "train_loader_kg = DataLoader(\n",
    "    PoetryDataset(train_df, use_kg=True),\n",
    "    batch_size=8,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader_kg = DataLoader(\n",
    "    PoetryDataset(test_df, use_kg=True),\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ”¥ KG ì‚¬ìš© í•™ìŠµ ì‹œì‘\")\n",
    "train_model(model, train_loader_kg)\n",
    "print(\"ğŸ“Š KG ì‚¬ìš© ì„±ëŠ¥\")\n",
    "evaluate(model, test_loader_kg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f21f6ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ ë¡œë“œ í›„ ìƒ˜í”Œ ìˆ˜: 945\n",
      "â“ ë¼ë²¨ ë¶„í¬ (ë¡œë“œ ì§í›„):\n",
      " label\n",
      "1    756\n",
      "0    189\n",
      "Name: count, dtype: int64\n",
      "âœ… ì •ì œ í›„ ìƒ˜í”Œ ìˆ˜: 945\n",
      "â“ ë¼ë²¨ ë¶„í¬ (ì •ì œ í›„):\n",
      " label\n",
      "1    756\n",
      "0    189\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ KG ë¯¸ì‚¬ìš© í•™ìŠµ ì‹œì‘\n",
      "Epoch 1 ì™„ë£Œ\n",
      "Epoch 2 ì™„ë£Œ\n",
      "Epoch 3 ì™„ë£Œ\n",
      "ğŸ“Š KG ë¯¸ì‚¬ìš© ê²°ê³¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.00      0.00      0.00        38\n",
      "          ai       0.80      1.00      0.89       151\n",
      "\n",
      "    accuracy                           0.80       189\n",
      "   macro avg       0.40      0.50      0.44       189\n",
      "weighted avg       0.64      0.80      0.71       189\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¥ KG ì‚¬ìš© í•™ìŠµ ì‹œì‘\n",
      "Epoch 1 ì™„ë£Œ\n",
      "Epoch 2 ì™„ë£Œ\n",
      "Epoch 3 ì™„ë£Œ\n",
      "ğŸ“Š KG ì‚¬ìš© ê²°ê³¼\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.00      0.00      0.00        38\n",
      "          ai       0.80      1.00      0.89       151\n",
      "\n",
      "    accuracy                           0.80       189\n",
      "   macro avg       0.40      0.50      0.44       189\n",
      "weighted avg       0.64      0.80      0.71       189\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# 1) ë°ì´í„° ë¡œë“œ\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "print(\"â“ ë¡œë“œ í›„ ìƒ˜í”Œ ìˆ˜:\", len(df))\n",
    "print(\"â“ ë¼ë²¨ ë¶„í¬ (ë¡œë“œ ì§í›„):\\n\", df['label'].value_counts())\n",
    "\n",
    "# 2) ë¬¸ìì—´ 'human'/'ai' â†’ 0/1 ë§¤í•‘ (ì´ë¯¸ ìˆ«ìë©´ ê±´ë„ˆë›°ê¸°)\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df['label'].dtype == object:\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "\n",
    "# 3) ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ & 0Â·1 ì™¸ ë‚˜ë¨¸ì§€(NaN í¬í•¨) ì‚­ì œ\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce').astype('Int64')\n",
    "df = df[df['label'].isin([0, 1])].reset_index(drop=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "print(\"âœ… ì •ì œ í›„ ìƒ˜í”Œ ìˆ˜:\", len(df))\n",
    "print(\"â“ ë¼ë²¨ ë¶„í¬ (ì •ì œ í›„):\\n\", df['label'].value_counts())\n",
    "\n",
    "# 4) KG ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = (\n",
    "    kg_df\n",
    "    .apply(lambda r: f\"{r['subject']} {r['predicate']} {r['object']}\", axis=1)\n",
    "    .tolist()\n",
    ")\n",
    "def find_relevant_kg(text, kg_texts, topk=3):\n",
    "    hits = [kg for kg in kg_texts if any(tok in kg for tok in text.split())]\n",
    "    return \" \".join(hits[:topk])\n",
    "\n",
    "# 5) Dataset ì •ì˜\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, use_kg=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        if self.use_kg:\n",
    "            kg = find_relevant_kg(text, kg_texts)\n",
    "            text = f\"{text} [SEP] {kg}\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.df.iloc[idx]['label'], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# 6) ë°ì´í„° ë¶„ë¦¬\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "# 7) í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ì¤€ë¹„\n",
    "model_name = 'monologg/kobert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cpu\")\n",
    "def get_model():\n",
    "    return BertForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "# 8) í•™ìŠµ/í‰ê°€ í•¨ìˆ˜\n",
    "def train_model(model, loader, epochs=3):\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "    model.train()\n",
    "    for e in range(epochs):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            loss = model(**batch).loss\n",
    "            loss.backward()\n",
    "            optim.step(); optim.zero_grad()\n",
    "        print(f\"Epoch {e+1} ì™„ë£Œ\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**batch).logits\n",
    "            preds += torch.argmax(logits, axis=1).cpu().tolist()\n",
    "            labs  += batch['labels'].cpu().tolist()\n",
    "    print(classification_report(labs, preds, target_names=['human','ai']))\n",
    "\n",
    "# 9) KG ë¯¸ì‚¬ìš©\n",
    "model = get_model()\n",
    "dl_train = DataLoader(PoetryDataset(train_df, tokenizer, use_kg=False),\n",
    "                      batch_size=8, shuffle=True)\n",
    "dl_test  = DataLoader(PoetryDataset(test_df,  tokenizer, use_kg=False),\n",
    "                      batch_size=8)\n",
    "print(\"ğŸ”¥ KG ë¯¸ì‚¬ìš© í•™ìŠµ ì‹œì‘\")\n",
    "train_model(model, dl_train)\n",
    "print(\"ğŸ“Š KG ë¯¸ì‚¬ìš© ê²°ê³¼\")\n",
    "evaluate(model, dl_test)\n",
    "\n",
    "# 10) KG ì‚¬ìš©\n",
    "model = get_model()\n",
    "dl_train = DataLoader(PoetryDataset(train_df, tokenizer, use_kg=True),\n",
    "                      batch_size=8, shuffle=True)\n",
    "dl_test  = DataLoader(PoetryDataset(test_df,  tokenizer, use_kg=True),\n",
    "                      batch_size=8)\n",
    "print(\"\\nğŸ”¥ KG ì‚¬ìš© í•™ìŠµ ì‹œì‘\")\n",
    "train_model(model, dl_train)\n",
    "print(\"ğŸ“Š KG ì‚¬ìš© ê²°ê³¼\")\n",
    "evaluate(model, dl_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
