# -*- coding: utf-8 -*-
"""
wiki_style_kg.py â€” í•œêµ­ì–´ 'ë¬¸ì²´Â·í‘œê¸°Â·ë§ì¶¤ë²•' ìœ„í‚¤ í˜ì´ì§€ â†’ ì‚¼ìš”ì†Œ íŠ¸ë¦¬í”Œ CSV

ğŸ”¹ ê°œì„  í¬ì¸íŠ¸
  â€¢ ì¹´í…Œê³ ë¦¬ ë¹„ì–´ ìˆìœ¼ë©´ ì˜ˆë¹„ í˜ì´ì§€ 30 ê°œ ì‚¬ìš©
  â€¢ ë¬¸ì¥ ê¸¸ì´ í•„í„° 10â€“200 ìë¡œ ì™„í™”, ì œëª© í¬í•¨ ì¡°ê±´ ì œê±°
  â€¢ predicate 3 ì¢…: defines / example_of / contrast_with
  â€¢ object  : Okt í˜•íƒœì†Œ + TF-IDF ìƒìœ„ 3 ëª…ì‚¬ í‚¤ì›Œë“œ
  â€¢ ì¤‘ë³µ íŠ¸ë¦¬í”Œ ì œê±°
  â€¢ triples.csv + README_LICENSE.txt ìë™ ìƒì„±

ğŸ”¹ ì˜ì¡´
  pip install "wikipedia-api<0.6" scikit-learn pandas tqdm konlpy==0.6.0
  sudo apt-get install -y default-jdk   # (Ubuntu, Konlpy JPype ìš©)
"""

import re, csv, pathlib
from typing import List, Tuple
import wikipediaapi, tqdm
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USER_AGENT = "AdvWikiKG/0.3 (https://example.com)"
WIKI = wikipediaapi.Wikipedia("ko", headers={"User-Agent": USER_AGENT})

CATEGORY  = "í•œêµ­ì–´ ë¬¸ì²´"     # ì¹´í…Œê³ ë¦¬ ë¨¼ì € ì‹œë„
MAX_PAGES = 30

LEN_MIN, LEN_MAX = 10, 200    # â† ê¸¸ì´ ì¡°ê±´ ì™„í™”
TOKEN_RE = r"[ê°€-í£A-Za-z]{2,}"

SENT_SPLIT = re.compile(r"[.!?]\s|\n")
okt = Okt()

PRED_PATTERNS = [
    (re.compile(r"ì˜ˆë¥¼ ë“¤ì–´"), "example_of"),
    (re.compile(r"[ì™€ê³¼] ë‹¤ë¥´ê²Œ"), "contrast_with"),
]
DEFAULT_PRED = "defines"

FALLBACK_PAGES = [
    # ê¸°ë³¸ 10ê°œ + ì¶”ê°€ 20ê°œ (ì¡´ì¬ ì—¬ë¶€ëŠ” ìœ„í‚¤ ê²€ìƒ‰ìœ¼ë¡œ í™•ì¸)
    "êµ¬ì–´ì²´", "ë¬¸ì–´ì²´", "ë†’ì„ë²•", "ê²½ì–´ë²•", "í‘œì¤€ì–´", "ë°©ì–¸",
    "ì¸í„°ë„· ì†ì–´", "ê¸ˆê¸°ì–´", "êµ­ì–´ ë¬¸ì¥ë¶€í˜¸", "ë§ì¶¤ë²•",
    "ì¡´ëŒ“ë§", "ë°˜ë§", "ê²©ì‹ì²´", "ì†ì–´", "ì‹ ì¡°ì–´",
    "ê´€ìš©êµ¬", "ë¹„ì†ì–´", "ì™¸ë˜ì–´", "ë„ì–´ì“°ê¸°", "ë‘ìŒ ë²•ì¹™",
    "ì‚¬íˆ¬ë¦¬", "ì˜ì„±ì–´", "ì˜íƒœì–´", "ì¢…ê²°ì–´ë¯¸", "ì˜ë¬¸ë¬¸",
    "ë¶€ì •ë¬¸", "í”¼ë™", "ì‚¬ë™", "ì¤‘ì˜ì  í‘œí˜„", "ë¡œë§ˆì í‘œê¸°ë²•"
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. í•¨ìˆ˜: ì¹´í…Œê³ ë¦¬ì—ì„œ í˜ì´ì§€ ìˆ˜ì§‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_pages(category: str, limit: int) -> List[Tuple[str, str]]:
    cat = WIKI.page(f"Category:{category}")
    pages = []
    for title, page in cat.categorymembers.items():
        if page.ns == 0:
            pages.append((title, page.fullurl))
        if len(pages) >= limit:
            break
    return pages

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. í˜ì´ì§€ ëª©ë¡ ê²°ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pages = collect_pages(CATEGORY, MAX_PAGES)

if not pages:
    print("âš ï¸  ì¹´í…Œê³ ë¦¬ ë¹„ì–´ ìˆìŒ â†’ ì˜ˆë¹„ í˜ì´ì§€ ì‚¬ìš©")
    pages = [
        (title, WIKI.page(title).fullurl)
        for title in FALLBACK_PAGES
        if WIKI.page(title).exists()
    ]

if not pages:
    raise SystemExit("âŒ  ì˜ˆë¹„ í˜ì´ì§€ë„ ëª¨ë‘ ì‹¤íŒ¨ â€” ì œëª©ì„ í™•ì¸í•˜ì„¸ìš”.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ë¬¸ì¥ ìˆ˜ì§‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sentences, meta = [], []   # sentence, (subject, url)
for subj, url in tqdm.tqdm(pages, desc="fetch pages"):
    page = WIKI.page(subj)
    if not page.exists():
        continue
    for sent in SENT_SPLIT.split(page.text):
        sent = sent.strip()
        if LEN_MIN < len(sent) < LEN_MAX:
            sentences.append(sent)
            meta.append((subj, url))

if len(sentences) < 300:
    raise SystemExit(f"â—  ìˆ˜ì§‘ ë¬¸ì¥ ìˆ˜ê°€ {len(sentences)}ê°œ â€” ë” ëŠ˜ë ¤ì•¼ í•©ë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
vectorizer = TfidfVectorizer(max_features=8000, token_pattern=TOKEN_RE)
X = vectorizer.fit_transform(sentences)
idx2term = {i: t for t, i in vectorizer.vocabulary_.items()}

triples, seen = [], set()
for i, sent in enumerate(sentences):
    subj, url = meta[i]

    # predicate ì„ íƒ
    pred = next((tag for pat, tag in PRED_PATTERNS if pat.search(sent)), DEFAULT_PRED)

    # TF-IDF ìƒìœ„ 3 ëª…ì‚¬
    row = X.getrow(i)
    if row.nnz == 0:
        continue
    top_idx = row.indices[row.data.argsort()[::-1][:3]]
    obj = " ".join(idx2term[j] for j in top_idx)
    if not obj:
        continue

    key = (subj, pred, obj)
    if key in seen:
        continue
    seen.add(key)
    triples.append([subj, pred, obj, url])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. CSV + ë¼ì´ì„ ìŠ¤ íŒŒì¼ ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
out_csv = pathlib.Path("triples.csv")
with out_csv.open("w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["subject", "predicate", "object", "src_url"])
    writer.writerows(triples)

readme = pathlib.Path("README_LICENSE.txt")
readme.write_text(
    "Data Source\n-----------\n"
    "ë³¸ íŠ¸ë¦¬í”Œì€ í•œêµ­ì–´ ìœ„í‚¤ë°±ê³¼ ë¤í”„(CC BY-SA 3.0)ì—ì„œ ìë™ ì¶”ì¶œëœ fact ì •ë³´ì…ë‹ˆë‹¤.\n"
    "ì›ë¬¸ ë¬¸ì¥ì€ í¬í•¨ë˜ì§€ ì•Šì•˜ìœ¼ë©°, ì¬ë°°í¬ ì‹œ CC BY-SA 3.0 ì¶œì²˜ ê³ ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    encoding="utf-8",
)

print(f"âœ“ {len(triples)} triples saved â†’ {out_csv.resolve()}")
