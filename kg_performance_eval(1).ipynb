{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8e31f4",
   "metadata": {},
   "source": [
    "# KG 성능 비교 실험 (ROUGE 기반 평가)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d3a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# 데이터 불러오기\n",
    "kg = pd.read_csv(\"kg_triples_test.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# 트리플 결합 텍스트 만들기\n",
    "kg[\"triple_text\"] = kg[\"subject\"] + \" \" + kg[\"predicate\"] + \" \" + kg[\"object\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac6233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 유사도 기반 KG 검색 함수\n",
    "def find_relevant_kg(text, kg_texts, top_k=3):\n",
    "    matches = []\n",
    "    for t in kg_texts:\n",
    "        score = sum([1 for w in t.split() if w in text])\n",
    "        matches.append((t, score))\n",
    "    matches.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [x[0] for x in matches[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55bbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 구성 함수\n",
    "def build_prompt(text, kg_hits):\n",
    "    context = \"\\n\".join(kg_hits)\n",
    "    return f\"다음은 참고 지식입니다:\\n{context}\\n\\n사용자 입력: {text}\\n답변:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d974c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE 평가 함수\n",
    "def evaluate_with_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, pred)[\"rougeL\"].fmeasure for pred, ref in zip(predictions, references)]\n",
    "    return sum(scores) / len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a94795",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'input'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m sample = test_df.sample(\u001b[32m10\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1. KG 없이 (baseline)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m preds_no_kg = [\u001b[33m\"\u001b[39m\u001b[33m답변: \u001b[39m\u001b[33m\"\u001b[39m + text[:\u001b[32m20\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. KG 사용\u001b[39;00m\n\u001b[32m     10\u001b[39m preds_with_kg = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'input'"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# 샘플 10개 추출\n",
    "sample = test_df.sample(10, random_state=42)\n",
    "\n",
    "# 1. KG 없이 (baseline)\n",
    "preds_no_kg = [\"답변: \" + text[:20] for text in sample[\"input\"]]\n",
    "\n",
    "# 2. KG 사용\n",
    "preds_with_kg = []\n",
    "for text in sample[\"input\"]:\n",
    "    hits = find_relevant_kg(text, kg[\"triple_text\"])\n",
    "    prompt = build_prompt(text, hits)\n",
    "    preds_with_kg.append(\"답변: \" + text[:20] + \" (지식반영)\")\n",
    "\n",
    "# 평가\n",
    "rouge_no_kg = evaluate_with_rouge(preds_no_kg, sample[\"output\"])\n",
    "rouge_with_kg = evaluate_with_rouge(preds_with_kg, sample[\"output\"])\n",
    "\n",
    "print(f\"KG 미사용 시 ROUGE-L: {rouge_no_kg:.4f}\")\n",
    "print(f\"KG 사용 시 ROUGE-L: {rouge_with_kg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22f11a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 질문: 라이스 페이퍼의 두께나 구성이 일반적이지 않아 조금 놀랐습니다. 아마, 식사량을 충분히 하는 것이 목표였을건데, 이로 인해 월남쌈의 질감이나 맛을 느끼는 부분에서 조금 힘들었습니다. 개인적으로는 훠궈 육수는 조금 아쉬움이 남았지만, 다른 메뉴들 중에서는 고기가 빼어난 맛을 보였습니다.\n",
      "\n",
      "[프롬프트 - KG 미사용]\n",
      " 질문: 라이스 페이퍼의 두께나 구성이 일반적이지 않아 조금 놀랐습니다. 아마, 식사량을 충분히 하는 것이 목표였을건데, 이로 인해 월남쌈의 질감이나 맛을 느끼는 부분에서 조금 힘들었습니다. 개인적으로는 훠궈 육수는 조금 아쉬움이 남았지만, 다른 메뉴들 중에서는 고기가 빼어난 맛을 보였습니다.\n",
      "답변:\n",
      "\n",
      "[프롬프트 - KG 사용]\n",
      " 질문: 라이스 페이퍼의 두께나 구성이 일반적이지 않아 조금 놀랐습니다. 아마, 식사량을 충분히 하는 것이 목표였을건데, 이로 인해 월남쌈의 질감이나 맛을 느끼는 부분에서 조금 힘들었습니다. 개인적으로는 훠궈 육수는 조금 아쉬움이 남았지만, 다른 메뉴들 중에서는 고기가 빼어난 맛을 보였습니다.\n",
      "배경지식:\n",
      "- 사용자 질문한다 다른 사람의 의도\n",
      "답변:\n",
      "\n",
      "🤖 [KG 미사용 답변]\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 지정된 파일을 찾을 수 없습니다",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# ✅ 8. Ollama 실행 및 출력 결과\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🤖 [KG 미사용 답변]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrun_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_no_kg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🤖 [KG 사용 답변]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(run_ollama(prompt_kg))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mrun_ollama\u001b[39m\u001b[34m(prompt, model)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_ollama\u001b[39m(prompt, model=\u001b[33m\"\u001b[39m\u001b[33mllama3\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     result = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mollama\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.stdout.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\subprocess.py:556\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstdout\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m    554\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstderr\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    558\u001b[39m         stdout, stderr = process.communicate(\u001b[38;5;28minput\u001b[39m, timeout=timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\subprocess.py:1038\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1034\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1035\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1036\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1048\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\User\\lsm40\\anaconda3\\envs\\min24\\Lib\\subprocess.py:1550\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[39m\n\u001b[32m   1548\u001b[39m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     hp, ht, pid, tid = \u001b[43m_winapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[32m   1552\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1554\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1559\u001b[39m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[32m   1560\u001b[39m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1563\u001b[39m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[32m   1564\u001b[39m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[32m   1565\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_pipe_fds(p2cread, p2cwrite,\n\u001b[32m   1566\u001b[39m                          c2pread, c2pwrite,\n\u001b[32m   1567\u001b[39m                          errread, errwrite)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] 지정된 파일을 찾을 수 없습니다"
     ]
    }
   ],
   "source": [
    "# ✅ 1. 필요한 라이브러리 로딩\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "# ✅ 2. Knowledge Graph 로딩 및 텍스트 구성\n",
    "kg = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg[\"triple_text\"] = kg[\"subject\"] + \" \" + kg[\"predicate\"] + \" \" + kg[\"object\"]\n",
    "\n",
    "# ✅ 3. 테스트 질문 로딩\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# ✅ 4. KG 검색 함수\n",
    "def find_relevant_kg(user_input, kg_texts, topk=3):\n",
    "    return [t for t in kg_texts if any(word in user_input for word in t.split())][:topk]\n",
    "\n",
    "# ✅ 5. 프롬프트 생성 함수\n",
    "def build_prompt(user_input, kg_hits=None):\n",
    "    prompt = f\"질문: {user_input}\\n\"\n",
    "    if kg_hits:\n",
    "        prompt += \"배경지식:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += \"답변:\"\n",
    "    return prompt\n",
    "\n",
    "# ✅ 6. 로컬 Ollama 모델 호출 함수 (예: llama3)\n",
    "def run_ollama(prompt, model=\"llama3\"):\n",
    "    result = subprocess.run(\n",
    "        [\"ollama\", \"run\", model],\n",
    "        input=prompt.encode(\"utf-8\"),\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        timeout=30\n",
    "    )\n",
    "    return result.stdout.decode(\"utf-8\").strip()\n",
    "\n",
    "# ✅ 7. 질문 입력 → KG 사용 여부에 따른 비교\n",
    "sample = test_df.sample(1, random_state=42)\n",
    "user_input = sample[\"sentence1\"].values[0]\n",
    "true_answer = sample[\"sentence2\"].values[0]\n",
    "\n",
    "kg_hits = find_relevant_kg(user_input, kg[\"triple_text\"])\n",
    "prompt_kg = build_prompt(user_input, kg_hits)\n",
    "prompt_no_kg = build_prompt(user_input)\n",
    "\n",
    "print(\"📌 질문:\", user_input)\n",
    "print(\"\\n[프롬프트 - KG 미사용]\\n\", prompt_no_kg)\n",
    "print(\"\\n[프롬프트 - KG 사용]\\n\", prompt_kg)\n",
    "\n",
    "# ✅ 8. Ollama 실행 및 출력 결과\n",
    "print(\"\\n🤖 [KG 미사용 답변]\\n\")\n",
    "print(run_ollama(prompt_no_kg))\n",
    "\n",
    "print(\"\\n🤖 [KG 사용 답변]\\n\")\n",
    "print(run_ollama(prompt_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53429b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 KG 미사용:\n",
      "프롬프트: 질문: 질문: 나는 [MASK] 기분이다\n",
      "답변: 질문: 질문: 나는 꽉 기분이다\n",
      "\n",
      "📌 KG 사용:\n",
      "프롬프트: 질문: 질문: 나는 [MASK] 기분이다\n",
      "답변: 질문: 질문: 나는 꽉 기분이다\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import pandas as pd\n",
    "\n",
    "# CPU로 강제 설정\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# KoBERT 모델과 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertForMaskedLM.from_pretrained('monologg/kobert')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# KG 데이터 불러오기\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "if 'triple_text' not in kg_df.columns:\n",
    "    kg_df['triple_text'] = kg_df['subject'] + \" \" + kg_df['predicate'] + \" \" + kg_df['object']\n",
    "\n",
    "# 배경지식 찾기\n",
    "def find_relevant_kg(question, kg_df):\n",
    "    hits = []\n",
    "    for triple in kg_df['triple_text']:\n",
    "        if any(word in triple for word in question.split()):\n",
    "            hits.append(triple)\n",
    "    return hits[:3]\n",
    "\n",
    "# 프롬프트 생성\n",
    "def build_prompt(question, kg_hits=None):\n",
    "    prompt = \"\"\n",
    "    if kg_hits:\n",
    "        prompt += \"배경지식:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += f\"질문: {question}\"\n",
    "    return prompt\n",
    "\n",
    "# 답변 생성\n",
    "def generate_answer(prompt):\n",
    "    if \"[MASK]\" not in prompt:\n",
    "        return \"질문에 [MASK]가 포함되어야 합니다.\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = outputs.logits[0, mask_token_index, :]\n",
    "    top_token = torch.argmax(mask_token_logits, dim=1)\n",
    "    predicted_token = tokenizer.decode(top_token)\n",
    "    \n",
    "    return prompt.replace(\"[MASK]\", predicted_token)\n",
    "\n",
    "# 사용자 입력\n",
    "user_question = input(\"질문을 입력하세요 (예: 나는 [MASK] 기분이다): \")\n",
    "\n",
    "# KG 미사용\n",
    "prompt_no_kg = build_prompt(user_question)\n",
    "answer_no_kg = generate_answer(prompt_no_kg)\n",
    "\n",
    "# KG 사용\n",
    "kg_hits = find_relevant_kg(user_question, kg_df)\n",
    "prompt_with_kg = build_prompt(user_question, kg_hits)\n",
    "answer_with_kg = generate_answer(prompt_with_kg)\n",
    "\n",
    "print(\"\\n KG 미사용:\")\n",
    "print(\"프롬프트:\", prompt_no_kg)\n",
    "print(\"답변:\", answer_no_kg)\n",
    "\n",
    "print(\"\\n KG 사용:\")\n",
    "print(\"프롬프트:\", prompt_with_kg)\n",
    "print(\"답변:\", answer_with_kg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 [KG 미사용 프롬프트]\n",
      " 질문: 피곤해\n",
      "답변:\n",
      "\n",
      "🤖 [KG 미사용 답변]\n",
      " ⏱️ 실행 시간이 초과되었습니다.\n",
      "\n",
      "📌 [KG 사용 프롬프트]\n",
      " 질문: 피곤해\n",
      "답변:\n",
      "\n",
      "🤖 [KG 사용 답변]\n",
      " ⏱️ 실행 시간이 초과되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "# Ollama 실행 경로 (직접 지정)\n",
    "OLLAMA_PATH = r\"C:\\Users\\lsm40\\AppData\\Local\\Programs\\Ollama\\ollama.exe\"\n",
    "\n",
    "# 사용자 질문 입력\n",
    "user_input = input(\"질문을 입력하세요: \")\n",
    "\n",
    "# KG 파일 불러오기\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")  # 경로는 필요에 따라 수정\n",
    "\n",
    "# KG에서 관련 정보 추출 (간단한 키워드 기반 검색)\n",
    "def find_relevant_kg(question, kg_df):\n",
    "    hits = []\n",
    "    for idx, row in kg_df.iterrows():\n",
    "        triple = f\"{row['subject']}, {row['predicate']}, {row['object']}\"\n",
    "        if any(word in triple for word in question.split()):\n",
    "            hits.append(triple)\n",
    "    return hits[:3]  # 최대 3개까지만 사용\n",
    "\n",
    "# 프롬프트 생성\n",
    "def build_prompt(user_input, kg_hits=None):\n",
    "    prompt = \"\"\n",
    "    if kg_hits:\n",
    "        prompt += \"배경지식:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += f\"질문: {user_input}\\n답변:\"\n",
    "    return prompt\n",
    "\n",
    "#  Llama3로 답변 생성\n",
    "def run_ollama(prompt, model=\"llama3\"):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [OLLAMA_PATH, \"run\", model],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=60\n",
    "        )\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except FileNotFoundError:\n",
    "        return \" Ollama 실행 파일을 찾을 수 없습니다. 경로를 다시 확인하세요.\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \" 실행 시간이 초과되었습니다.\"\n",
    "    except Exception as e:\n",
    "        return f\" 오류 발생: {e}\"\n",
    "\n",
    "#  KG 검색 결과 가져오기\n",
    "kg_hits = find_relevant_kg(user_input, kg_df)\n",
    "\n",
    "#  프롬프트 생성\n",
    "prompt_kg = build_prompt(user_input, kg_hits)\n",
    "prompt_no_kg = build_prompt(user_input)\n",
    "\n",
    "#  출력\n",
    "print(\"\\n [KG 미사용 프롬프트]\\n\", prompt_no_kg)\n",
    "print(\"\\n [KG 미사용 답변]\\n\", run_ollama(prompt_no_kg))\n",
    "\n",
    "print(\"\\n [KG 사용 프롬프트]\\n\", prompt_kg)\n",
    "print(\"\\n [KG 사용 답변]\\n\", run_ollama(prompt_kg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51647132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 [KG 미사용 프롬프트]\n",
      " 질문: 나 피곤해\n",
      "답변:\n",
      "\n",
      "🤖 [KG 미사용 답변]\n",
      " ⏱️ 실행 시간이 초과되었습니다.\n",
      "\n",
      "📌 [KG 사용 프롬프트]\n",
      " 질문: 나 피곤해\n",
      "답변:\n",
      "\n",
      "🤖 [KG 사용 답변]\n",
      " ⏱️ 실행 시간이 초과되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#  Ollama 실행 파일 경로 (직접 확인된 경로로 수정 필요)\n",
    "OLLAMA_PATH = r\"C:\\Users\\lsm40\\AppData\\Local\\Programs\\Ollama\\ollama.exe\"\n",
    "\n",
    "#  사용자 질문 실시간 입력\n",
    "user_input = input(\"질문을 입력하세요: \").strip()\n",
    "\n",
    "#  KG 그래프 CSV 로딩 (컬럼명 자동 감지)\n",
    "kg_path = \"./kg_triples_test.csv\"  # 예: 'triple_text' 열이 있는 파일\n",
    "kg_df = pd.read_csv(kg_path)\n",
    "col = kg_df.columns[0]  # 첫 번째 열 자동 인식\n",
    "\n",
    "#  KG에서 관련된 배경지식 추출 함수\n",
    "def find_relevant_kg(question, df, column):\n",
    "    hits = []\n",
    "    for triple in df[column]:\n",
    "        if any(word in triple for word in question.split()):\n",
    "            hits.append(triple)\n",
    "    return hits[:3]\n",
    "\n",
    "#  프롬프트 생성 함수\n",
    "def build_prompt(question, kg_hits=None):\n",
    "    prompt = \"\"\n",
    "    if kg_hits:\n",
    "        prompt += \"배경지식:\\n\"\n",
    "        for hit in kg_hits:\n",
    "            prompt += f\"- {hit}\\n\"\n",
    "    prompt += f\"질문: {question}\\n답변:\"\n",
    "    return prompt\n",
    "\n",
    "#  Llama3 실행 함수\n",
    "def run_ollama(prompt, timeout=60):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [OLLAMA_PATH, \"run\", \"llama3\"],\n",
    "            input=prompt.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        if result.stderr:\n",
    "            print(\" stderr:\", result.stderr.decode(\"utf-8\").strip())\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \" 실행 시간이 초과되었습니다.\"\n",
    "    except FileNotFoundError:\n",
    "        return \" Ollama 실행 파일을 찾을 수 없습니다. 경로를 확인하세요.\"\n",
    "\n",
    "#  KG 기반 프롬프트 구성\n",
    "kg_hits = find_relevant_kg(user_input, kg_df, col)\n",
    "prompt_no_kg = build_prompt(user_input)\n",
    "prompt_kg = build_prompt(user_input, kg_hits)\n",
    "\n",
    "#  실행 및 결과 출력\n",
    "print(\"\\n [KG 미사용 프롬프트]\\n\", prompt_no_kg)\n",
    "print(\"\\n [KG 미사용 답변]\\n\", run_ollama(prompt_no_kg))\n",
    "\n",
    "print(\"\\n [KG 사용 프롬프트]\\n\", prompt_kg)\n",
    "print(\"\\n [KG 사용 답변]\\n\", run_ollama(prompt_kg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e98a60b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training without KG\n",
      "Epoch 1 complete\n",
      "Epoch 2 complete\n",
      "Epoch 3 complete\n",
      "Evaluation without KG\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.00      0.00      0.00        38\n",
      "          ai       0.80      1.00      0.89       151\n",
      "\n",
      "    accuracy                           0.80       189\n",
      "   macro avg       0.40      0.50      0.44       189\n",
      "weighted avg       0.64      0.80      0.71       189\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with KG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete\n",
      "Evaluation with KG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.00      0.00      0.00        38\n",
      "          ai       0.80      1.00      0.89       151\n",
      "\n",
      "    accuracy                           0.80       189\n",
      "   macro avg       0.40      0.50      0.44       189\n",
      "weighted avg       0.64      0.80      0.71       189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "model_name = 'monologg/kobert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 라벨 처리\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df['label'].dtype == object:\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "df = df.dropna(subset=['label'])\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# KG 로드\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = [\n",
    "    f\"{r['subject']} {r['predicate']} {r['object']}\"\n",
    "    for _, r in kg_df.iterrows()\n",
    "]\n",
    "\n",
    "def find_relevant_kg(text, kg_texts, topk=3):\n",
    "    hits = [kg for kg in kg_texts if any(tok in kg for tok in text.split())]\n",
    "    return \" \".join(hits[:topk])\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['text']\n",
    "        kg = find_relevant_kg(text, kg_texts) if self.use_kg else None\n",
    "\n",
    "        # 수정된 부분: truncation=True 로 통일\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            kg,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in encoded.items()}\n",
    "        item['labels'] = torch.tensor(row['label'], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "def get_model():\n",
    "    return BertForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "def train_model(model, loader, epochs=3):\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            outputs.loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1} complete\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**batch).logits\n",
    "            preds += torch.argmax(logits, axis=1).cpu().tolist()\n",
    "            labels += batch['labels'].cpu().tolist()\n",
    "    print(classification_report(labels, preds, target_names=[\"human\", \"ai\"], zero_division=0))\n",
    "\n",
    "# KG 미사용\n",
    "model = get_model()\n",
    "train_loader = DataLoader(\n",
    "    PoetryDataset(train_df, tokenizer, use_kg=False),\n",
    "    batch_size=8, shuffle=True, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    PoetryDataset(test_df, tokenizer, use_kg=False),\n",
    "    batch_size=8, shuffle=False, pin_memory=True\n",
    ")\n",
    "print(\"Training without KG\")\n",
    "train_model(model, train_loader, epochs=3)\n",
    "print(\"Evaluation without KG\")\n",
    "evaluate(model, test_loader)\n",
    "\n",
    "# KG 사용\n",
    "model = get_model()\n",
    "train_loader = DataLoader(\n",
    "    PoetryDataset(train_df, tokenizer, use_kg=True),\n",
    "    batch_size=8, shuffle=True, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    PoetryDataset(test_df, tokenizer, use_kg=True),\n",
    "    batch_size=8, shuffle=False, pin_memory=True\n",
    ")\n",
    "print(\"Training with KG\")\n",
    "train_model(model, train_loader, epochs=3)\n",
    "print(\"Evaluation with KG\")\n",
    "evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f21f6ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 로드 후 샘플 수: 945\n",
      " 라벨 분포 (로드 직후):\n",
      " label\n",
      "1    756\n",
      "0    189\n",
      "Name: count, dtype: int64\n",
      " 정제 후 샘플 수: 945\n",
      " 라벨 분포 (정제 후):\n",
      " label\n",
      "1    756\n",
      "0    189\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " KG 미사용 학습 시작\n",
      "Epoch 1 완료\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m dl_test  \u001b[38;5;241m=\u001b[39m DataLoader(PoetryDataset(test_df,  tokenizer, use_kg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    111\u001b[0m                       batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m KG 미사용 학습 시작\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m KG 미사용 결과\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m evaluate(model, dl_test)\n",
      "Cell \u001b[0;32mIn[18], line 91\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, loader, epochs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     90\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     optim\u001b[38;5;241m.\u001b[39mstep(); optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 완료\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/cuda-env/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cuda-env/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cuda-env/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# 1) 데이터 로드\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "print(\" 로드 후 샘플 수:\", len(df))\n",
    "print(\" 라벨 분포 (로드 직후):\\n\", df['label'].value_counts())\n",
    "\n",
    "# 2) 문자열 'human'/'ai' → 0/1 매핑 (이미 숫자면 건너뛰기)\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df['label'].dtype == object:\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "\n",
    "# 3) 숫자형으로 변환 & 0·1 외 나머지(NaN 포함) 삭제\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce').astype('Int64')\n",
    "df = df[df['label'].isin([0, 1])].reset_index(drop=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "print(\" 정제 후 샘플 수:\", len(df))\n",
    "print(\" 라벨 분포 (정제 후):\\n\", df['label'].value_counts())\n",
    "\n",
    "# 4) KG 불러오기\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = (\n",
    "    kg_df\n",
    "    .apply(lambda r: f\"{r['subject']} {r['predicate']} {r['object']}\", axis=1)\n",
    "    .tolist()\n",
    ")\n",
    "def find_relevant_kg(text, kg_texts, topk=3):\n",
    "    hits = [kg for kg in kg_texts if any(tok in kg for tok in text.split())]\n",
    "    return \" \".join(hits[:topk])\n",
    "\n",
    "# 5) Dataset 정의\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, use_kg=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        if self.use_kg:\n",
    "            kg = find_relevant_kg(text, kg_texts)\n",
    "            text = f\"{text} [SEP] {kg}\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.df.iloc[idx]['label'], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# 6) 데이터 분리\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "# 7) 토크나이저 및 모델 준비\n",
    "model_name = 'monologg/kobert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cpu\")\n",
    "def get_model():\n",
    "    return BertForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2\n",
    "    ).to(device)\n",
    "\n",
    "# 8) 학습/평가 함수\n",
    "def train_model(model, loader, epochs=3):\n",
    "    optim = AdamW(model.parameters(), lr=5e-5)\n",
    "    model.train()\n",
    "    for e in range(epochs):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            loss = model(**batch).loss\n",
    "            loss.backward()\n",
    "            optim.step(); optim.zero_grad()\n",
    "        print(f\"Epoch {e+1} 완료\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**batch).logits\n",
    "            preds += torch.argmax(logits, axis=1).cpu().tolist()\n",
    "            labs  += batch['labels'].cpu().tolist()\n",
    "    print(classification_report(labs, preds, target_names=['human','ai']))\n",
    "\n",
    "# 9) KG 미사용\n",
    "model = get_model()\n",
    "dl_train = DataLoader(PoetryDataset(train_df, tokenizer, use_kg=False),\n",
    "                      batch_size=8, shuffle=True)\n",
    "dl_test  = DataLoader(PoetryDataset(test_df,  tokenizer, use_kg=False),\n",
    "                      batch_size=8)\n",
    "print(\" KG 미사용 학습 시작\")\n",
    "train_model(model, dl_train)\n",
    "print(\" KG 미사용 결과\")\n",
    "evaluate(model, dl_test)\n",
    "\n",
    "# 10) KG 사용\n",
    "model = get_model()\n",
    "dl_train = DataLoader(PoetryDataset(train_df, tokenizer, use_kg=True),\n",
    "                      batch_size=8, shuffle=True)\n",
    "dl_test  = DataLoader(PoetryDataset(test_df,  tokenizer, use_kg=True),\n",
    "                      batch_size=8)\n",
    "print(\"\\n KG 사용 학습 시작\")\n",
    "train_model(model, dl_train)\n",
    "print(\" KG 사용 결과\")\n",
    "evaluate(model, dl_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62522105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 최종 샘플 수: 945\n",
      " 라벨 분포:\n",
      " label\n",
      "1    756\n",
      "0    189\n",
      "Name: count, dtype: int64\n",
      " Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[실험 1] KG 미사용 학습 시작\n",
      "Epoch 1/5 완료\n",
      "Epoch 2/5 완료\n",
      "Epoch 3/5 완료\n",
      "Epoch 4/5 완료\n",
      "Epoch 5/5 완료\n",
      "\n",
      " KG 미사용 평가 결과\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.41      0.79      0.54        38\n",
      "          ai       0.93      0.72      0.81       151\n",
      "\n",
      "    accuracy                           0.73       189\n",
      "   macro avg       0.67      0.75      0.67       189\n",
      "weighted avg       0.83      0.73      0.76       189\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[실험 2] KG 사용 학습 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 완료\n",
      "Epoch 2/5 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 완료\n",
      "Epoch 5/5 완료\n",
      "\n",
      " KG 사용 평가 결과\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.36      0.82      0.50        38\n",
      "          ai       0.93      0.64      0.76       151\n",
      "\n",
      "    accuracy                           0.67       189\n",
      "   macro avg       0.65      0.73      0.63       189\n",
      "weighted avg       0.82      0.67      0.70       189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import json\n",
    "\n",
    "# 1) 데이터 로드 함수\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# 2) 데이터 불러와서 DataFrame 생성\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 3) 라벨 정제\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df['label'].dtype == object:\n",
    "    df['label'] = df['label'].map(label_map)\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce').astype('Int64')\n",
    "df = df[df['label'].isin([0, 1])].reset_index(drop=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "print(\" 최종 샘플 수:\", len(df))\n",
    "print(\" 라벨 분포:\\n\", df['label'].value_counts())\n",
    "\n",
    "# 4) KG 불러오기\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = [\n",
    "    f\"{r['subject']} {r['predicate']} {r['object']}\"\n",
    "    for _, r in kg_df.iterrows()\n",
    "]\n",
    "\n",
    "def find_relevant_kg(text, kg_texts, topk=3):\n",
    "    hits = [kg for kg in kg_texts if any(tok in kg for tok in text.split())]\n",
    "    return \" \".join(hits[:topk])\n",
    "\n",
    "# 5) Dataset 정의\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, 'text']\n",
    "        kg_text = find_relevant_kg(text, kg_texts) if self.use_kg else None\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            kg_text,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            truncation='only_first' if kg_text else True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.df.loc[idx, 'label'], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# 6) train/test split\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "# 7) GPU 세팅\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\" Using device:\", device)\n",
    "\n",
    "# 8) tokenizer & model 함수\n",
    "model_name = 'monologg/kobert'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def get_model():\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# 9) Oversampling sampler\n",
    "counts = train_df['label'].value_counts().sort_index().tolist()  # [#human, #ai]\n",
    "sample_weights = [1.0 / counts[label] for label in train_df['label']]\n",
    "sampler = WeightedRandomSampler(\n",
    "    sample_weights, num_samples=len(sample_weights), replacement=True\n",
    ")\n",
    "\n",
    "# 10) DataLoader 생성 함수\n",
    "def make_train_loader(df, use_kg):\n",
    "    return DataLoader(\n",
    "        PoetryDataset(df, tokenizer, use_kg=use_kg),\n",
    "        batch_size=8,\n",
    "        sampler=sampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "def make_test_loader(df, use_kg):\n",
    "    return DataLoader(\n",
    "        PoetryDataset(df, tokenizer, use_kg=use_kg),\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "train_loader_no_kg = make_train_loader(train_df, use_kg=False)\n",
    "test_loader_no_kg  = make_test_loader(test_df,  use_kg=False)\n",
    "train_loader_kg    = make_train_loader(train_df, use_kg=True)\n",
    "test_loader_kg     = make_test_loader(test_df,  use_kg=True)\n",
    "\n",
    "# 11) 학습/평가 함수\n",
    "def train_model(model, loader, epochs=5):\n",
    "    class_weights = torch.tensor([counts[1], counts[0]], dtype=torch.float).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != 'labels'}).logits\n",
    "            loss = loss_fn(logits, batch['labels'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} 완료\")\n",
    "\n",
    "def evaluate(model, loader, title=\"\"):\n",
    "    print(f\"\\n {title} 평가 결과\")\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != 'labels'}).logits\n",
    "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            labs  += batch['labels'].cpu().tolist()\n",
    "    print(classification_report(labs, preds, target_names=['human','ai'], zero_division=0))\n",
    "\n",
    "# 12) 실험 1: KG 미사용\n",
    "model = get_model()\n",
    "print(\"\\n[실험 1] KG 미사용 학습 시작\")\n",
    "train_model(model, train_loader_no_kg, epochs=5)\n",
    "evaluate(model, test_loader_no_kg, title=\"KG 미사용\")\n",
    "\n",
    "# 13) 실험 2: KG 사용\n",
    "model = get_model()\n",
    "print(\"\\n[실험 2] KG 사용 학습 시작\")\n",
    "train_model(model, train_loader_kg, epochs=5)\n",
    "evaluate(model, test_loader_kg, title=\"KG 사용\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1e64601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KG 미사용] 학습 시작\n",
      "Epoch 1 완료\n",
      "Epoch 2 완료\n",
      "Epoch 3 완료\n",
      "Epoch 4 완료\n",
      "Epoch 5 완료\n",
      "[KG 미사용] 평가 결과\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.75      0.08      0.14        38\n",
      "          ai       0.81      0.99      0.89       151\n",
      "\n",
      "    accuracy                           0.81       189\n",
      "   macro avg       0.78      0.54      0.52       189\n",
      "weighted avg       0.80      0.81      0.74       189\n",
      "\n",
      "\n",
      "[KG 사용] 학습 시작\n",
      "Epoch 1 완료\n",
      "Epoch 2 완료\n",
      "Epoch 3 완료\n",
      "Epoch 4 완료\n",
      "Epoch 5 완료\n",
      "[KG 사용] 평가 결과\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human       0.50      0.11      0.17        38\n",
      "          ai       0.81      0.97      0.89       151\n",
      "\n",
      "    accuracy                           0.80       189\n",
      "   macro avg       0.66      0.54      0.53       189\n",
      "weighted avg       0.75      0.80      0.74       189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# 경고 메시지 최소화\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# 1) JSONL 파일 로드\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2) 라벨 인코딩\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df[\"label\"].dtype == object:\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# 3) KG 트리플 로드 및 TF-IDF 준비\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = kg_df.apply(\n",
    "    lambda r: f\"{r['subject']} {r['predicate']} {r['object']}\",\n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "kg_tfidf = tfidf.fit_transform(kg_texts)\n",
    "\n",
    "# 4) TF-IDF 기반 상위 3개 KG 검색\n",
    "def find_relevant_kg(text, topk=3):\n",
    "    vec = tfidf.transform([text])\n",
    "    sims = cosine_similarity(vec, kg_tfidf)[0]\n",
    "    idxs = sims.argsort()[-topk:][::-1]\n",
    "    return \" \".join(kg_texts[i] for i in idxs)\n",
    "\n",
    "# 5) 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 6) Dataset 정의 (항상 max_length로 패딩/트렁크)\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        kg   = find_relevant_kg(text) if self.use_kg else None\n",
    "\n",
    "        # ➤ text, kg 둘 다 max_length=128로 패딩/자르기\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            kg,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0).clone() for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.df.loc[idx, \"label\"], dtype=torch.long).clone()\n",
    "        return item\n",
    "\n",
    "# 7) 학습/테스트 분할\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# 8) 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 9) 모델 로드 함수\n",
    "def get_model():\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"monologg/kobert\", num_labels=2\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# 10) 학습 및 평가 함수\n",
    "def train_model(model, loader, epochs=5):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.2 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            loss = loss_fn(logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1} 완료\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            labs  += batch[\"labels\"].cpu().tolist()\n",
    "    print(classification_report(labs, preds, target_names=[\"human\",\"ai\"], zero_division=0))\n",
    "\n",
    "# 11) KG 미사용 vs KG 사용 실험\n",
    "for use_kg in (False, True):\n",
    "    model = get_model()\n",
    "    train_loader = DataLoader(\n",
    "        PoetryDataset(train_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=True, num_workers=4, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        PoetryDataset(test_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=False, num_workers=4, pin_memory=True\n",
    "    )\n",
    "    mode = \"KG 미사용\" if not use_kg else \"KG 사용\"\n",
    "    print(f\"\\n[{mode}] 학습 시작\")\n",
    "    train_model(model, train_loader, epochs=5)\n",
    "    print(f\"[{mode}] 평가 결과\")\n",
    "    evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fc537f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KG 미사용] 학습 시작\n",
      "Epoch 1 완료\n",
      "Epoch 2 완료\n",
      "Epoch 3 완료\n",
      "Epoch 4 완료\n",
      "Epoch 5 완료\n",
      "[KG 미사용] 평가 결과\n",
      "              precision    recall  f1-score     support\n",
      "human          0.538462  0.184211  0.274510   38.000000\n",
      "ai             0.823864  0.960265  0.886850  151.000000\n",
      "accuracy       0.804233  0.804233  0.804233    0.804233\n",
      "macro avg      0.681163  0.572238  0.580680  189.000000\n",
      "weighted avg   0.766481  0.804233  0.763734  189.000000\n",
      "\n",
      "[KG 사용] 학습 시작\n",
      "Epoch 1 완료\n",
      "Epoch 2 완료\n",
      "Epoch 3 완료\n",
      "Epoch 4 완료\n",
      "Epoch 5 완료\n",
      "[KG 사용] 평가 결과\n",
      "              precision    recall  f1-score    support\n",
      "human          0.400000  0.105263  0.166667   38.00000\n",
      "ai             0.810056  0.960265  0.878788  151.00000\n",
      "accuracy       0.788360  0.788360  0.788360    0.78836\n",
      "macro avg      0.605028  0.532764  0.522727  189.00000\n",
      "weighted avg   0.727611  0.788360  0.735610  189.00000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# 경고 메시지 숨기기\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# 1) JSONL 로드 함수\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# 2) 데이터 읽기 및 라벨 인코딩\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df[\"label\"].dtype == object:\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# 3) KG 트리플 로드 및 TF-IDF 인덱스 생성\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = kg_df.apply(\n",
    "    lambda r: f\"{r['subject']} {r['predicate']} {r['object']}\",\n",
    "    axis=1\n",
    ").tolist()\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "kg_tfidf = tfidf.fit_transform(kg_texts)\n",
    "\n",
    "# 4) TF-IDF + 코사인 유사도로 KG 검색\n",
    "def find_relevant_kg(text, topk=3):\n",
    "    query_vec = tfidf.transform([text])\n",
    "    sims = cosine_similarity(query_vec, kg_tfidf)[0]\n",
    "    idxs = sims.argsort()[-topk:][::-1]\n",
    "    return \" \".join(kg_texts[i] for i in idxs)\n",
    "\n",
    "# 5) KoBERT 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 6) PyTorch Dataset 정의\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"text\"]\n",
    "        kg = find_relevant_kg(text) if self.use_kg else None\n",
    "\n",
    "        # 본문과 KG를 모두 max_length=128로 패딩/트렁케이트\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            kg,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0).clone() for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(row[\"label\"], dtype=torch.long).clone()\n",
    "        return item\n",
    "\n",
    "# 7) 학습/테스트 분할\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# 8) 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 9) 모델 로드 함수\n",
    "def get_model():\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"monologg/kobert\",\n",
    "        num_labels=2\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# 10) 학습 함수\n",
    "def train_model(model, loader, epochs=5):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.2 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            loss = loss_fn(logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch+1} 완료\")\n",
    "\n",
    "# 11) 평가 함수 (전체 리포트 보여주기)\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            labs  += batch[\"labels\"].cpu().tolist()\n",
    "\n",
    "    # pandas 출력 옵션: 모든 열과 넓이를 보여 줌\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "    # classification_report를 dict로 받아 DataFrame으로 출력\n",
    "    report_dict = classification_report(labs, preds, target_names=[\"human\",\"ai\"],\n",
    "                                        output_dict=True, digits=4)\n",
    "    df_report = pd.DataFrame(report_dict).T\n",
    "    print(df_report)\n",
    "\n",
    "# 12) KG 미사용 vs KG 사용 실험\n",
    "for use_kg in (False, True):\n",
    "    model = get_model()\n",
    "    train_loader = DataLoader(\n",
    "        PoetryDataset(train_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=True,\n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        PoetryDataset(test_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=False,\n",
    "        num_workers=4, pin_memory=True\n",
    "    )\n",
    "    mode = \"KG 미사용\" if not use_kg else \"KG 사용\"\n",
    "    print(f\"\\n[{mode}] 학습 시작\")\n",
    "    train_model(model, train_loader, epochs=5)\n",
    "    print(f\"[{mode}] 평가 결과\")\n",
    "    evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b1cd228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KG 미사용] 학습 시작\n",
      "Epoch 1 완료\n",
      "Epoch 2 완료\n",
      "Epoch 3 완료\n",
      "Epoch 4 완료\n",
      "Epoch 5 완료\n",
      "[KG 미사용] 평가 결과\n",
      "              precision    recall  f1-score     support\n",
      "human          0.336634  0.894737  0.489209   38.000000\n",
      "ai             0.954545  0.556291  0.702929  151.000000\n",
      "accuracy       0.624339  0.624339  0.624339    0.624339\n",
      "macro avg      0.645590  0.725514  0.596069  189.000000\n",
      "weighted avg   0.830309  0.624339  0.659959  189.000000\n",
      "\n",
      "[KG 사용] 학습 시작\n",
      "Epoch 1 완료\n",
      "Epoch 2 완료\n",
      "Epoch 3 완료\n",
      "Epoch 4 완료\n",
      "Epoch 5 완료\n",
      "[KG 사용] 평가 결과\n",
      "              precision    recall  f1-score     support\n",
      "human          0.282443  0.973684  0.437870   38.000000\n",
      "ai             0.982759  0.377483  0.545455  151.000000\n",
      "accuracy       0.497354  0.497354  0.497354    0.497354\n",
      "macro avg      0.632601  0.675584  0.491662  189.000000\n",
      "weighted avg   0.841954  0.497354  0.523824  189.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import json\n",
    "\n",
    "# 경고 메시지 최소화\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# 1) JSONL 파일 로드\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2) 라벨 인코딩\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df[\"label\"].dtype == object:\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# 3) KG 트리플 로드 및 TF-IDF 인덱스 생성\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = kg_df.apply(\n",
    "    lambda r: f\"{r['subject']} {r['predicate']} {r['object']}\",\n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "kg_tfidf = tfidf.fit_transform(kg_texts)\n",
    "\n",
    "# 4) TF-IDF + 코사인 유사도로 top3 KG 검색\n",
    "def find_relevant_kg(text, topk=3):\n",
    "    qv = tfidf.transform([text])\n",
    "    sims = cosine_similarity(qv, kg_tfidf)[0]\n",
    "    idxs = sims.argsort()[-topk:][::-1]\n",
    "    return \" \".join(kg_texts[i] for i in idxs)\n",
    "\n",
    "# 5) KoBERT 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "\n",
    "# 6) Dataset 정의 (padding+truncation 통일)\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, df, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"text\"]\n",
    "        kg   = find_relevant_kg(text) if self.use_kg else None\n",
    "\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            kg,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0).clone() for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(row[\"label\"], dtype=torch.long).clone()\n",
    "        return item\n",
    "\n",
    "# 7) 학습/테스트 분할\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# 8) oversampling 위한 WeightedRandomSampler 생성\n",
    "counts = train_df[\"label\"].value_counts().sort_index().tolist()  # [human_count, ai_count]\n",
    "# 각 샘플에 inverse frequency 가중치 부여\n",
    "sample_weights = [1.0 / counts[label] for label in train_df[\"label\"]]\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# 9) 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 10) 모델 로드 함수\n",
    "def get_model():\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"monologg/kobert\", num_labels=2\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# 11) 학습 함수 (클래스 가중치 + 스케줄러)\n",
    "def train_model(model, loader, epochs=5):\n",
    "    # 클래스 가중치는 [ai_count, human_count] 순이 되도록\n",
    "    class_weights = torch.tensor([counts[1], counts[0]], dtype=torch.float).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.2 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            loss = loss_fn(logits, batch[\"labels\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch} 완료\")\n",
    "\n",
    "# 12) 평가 함수 (전체 리포트 출력)\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**{k: v for k, v in batch.items() if k != \"labels\"}).logits\n",
    "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            labs  += batch[\"labels\"].cpu().tolist()\n",
    "\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.width\", 1000)\n",
    "    report = classification_report(\n",
    "        labs, preds,\n",
    "        target_names=[\"human\",\"ai\"],\n",
    "        output_dict=True,\n",
    "        digits=4\n",
    "    )\n",
    "    print(pd.DataFrame(report).T)\n",
    "\n",
    "# 13) KG 미사용 vs KG 사용 실험\n",
    "for use_kg in (False, True):\n",
    "    model = get_model()\n",
    "    train_loader = DataLoader(\n",
    "        PoetryDataset(train_df, use_kg=use_kg),\n",
    "        batch_size=16,\n",
    "        sampler=sampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        PoetryDataset(test_df, use_kg=use_kg),\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    mode = \"KG 미사용\" if not use_kg else \"KG 사용\"\n",
    "    print(f\"\\n[{mode}] 학습 시작\")\n",
    "    train_model(model, train_loader, epochs=5)\n",
    "    print(f\"[{mode}] 평가 결과\")\n",
    "    evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7957ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KG 없이] 실험 시작\n",
      "Epoch 1 완료\n",
      "Epoch 2 완료\n",
      "Epoch 3 완료\n",
      "Epoch 4 완료\n",
      "Epoch 5 완료\n",
      "[KG 없이] 평가 결과\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ficl/cuda-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ficl/cuda-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ficl/cuda-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human     0.0000    0.0000    0.0000        38\n",
      "          ai     0.7989    1.0000    0.8882       151\n",
      "\n",
      "    accuracy                         0.7989       189\n",
      "   macro avg     0.3995    0.5000    0.4441       189\n",
      "weighted avg     0.6383    0.7989    0.7096       189\n",
      "\n",
      "\n",
      "[KG 포함] 실험 시작\n",
      "Epoch 1 완료\n",
      "Epoch 2 완료\n",
      "Epoch 3 완료\n",
      "Epoch 4 완료\n",
      "Epoch 5 완료\n",
      "[KG 포함] 평가 결과\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       human     0.5000    0.0263    0.0500        38\n",
      "          ai     0.8021    0.9934    0.8876       151\n",
      "\n",
      "    accuracy                         0.7989       189\n",
      "   macro avg     0.6511    0.5098    0.4688       189\n",
      "weighted avg     0.7414    0.7989    0.7192       189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# 경고 메시지 최소화\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# 1) 데이터 로드 및 전처리\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "data = load_jsonl(\"poetry.jsonl\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "label_map = {\"human\": 0, \"ai\": 1}\n",
    "if df[\"label\"].dtype == object:\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "df = df.dropna(subset=[\"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# 2) KG 트리플 로드 및 TF-IDF 준비\n",
    "kg_df = pd.read_csv(\"kg_triples_test.csv\")\n",
    "kg_texts = kg_df.apply(\n",
    "    lambda r: f\"{r['subject']} {r['predicate']} {r['object']}\",\n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf.fit(kg_texts)\n",
    "\n",
    "# 3) KG 검색 함수 ( 코사인 유사도 )\n",
    "def find_relevant_kg(text, topk=3):\n",
    "    qv = tfidf.transform([text])\n",
    "    sims = cosine_similarity(qv, tfidf.transform(kg_texts))[0]\n",
    "    idxs = sims.argsort()[-topk:][::-1]\n",
    "    return \" \".join(kg_texts[i] for i in idxs)\n",
    "\n",
    "# 4) Dataset 정의 (late-fusion)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/kobert\")\n",
    "tfidf_dim = len(tfidf.vocabulary_)\n",
    "\n",
    "class LateFusionDataset(Dataset): # 모델 KoBERT\n",
    "    def __init__(self, df, use_kg=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.use_kg = use_kg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"text\"]\n",
    "        kg_text = find_relevant_kg(text) if self.use_kg else \"\"\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
    "        kg_vec = torch.tensor(tfidf.transform([kg_text]).toarray()[0], dtype=torch.float)\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "        return input_ids, attention_mask, kg_vec, label\n",
    "\n",
    "# 5) Late-fusion 모델 정의\n",
    "class LateFusionModel(nn.Module):\n",
    "    def __init__(self, tfidf_dim):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.kg_mlp = nn.Sequential(\n",
    "            nn.Linear(tfidf_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, kg_vec):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_pool = out.pooler_output                 # (batch, hidden)\n",
    "        kg_emb = self.kg_mlp(kg_vec)                  # (batch, hidden)\n",
    "        fusion = torch.cat([text_pool, kg_emb], dim=1)  # (batch, hidden*2)\n",
    "        logits = self.classifier(fusion)              # (batch, 2)\n",
    "        return logits\n",
    "\n",
    "# 6) 학습·평가 함수\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model, loader, epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        for input_ids, attention_mask, kg_vec, labels in loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            kg_vec = kg_vec.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(input_ids, attention_mask, kg_vec)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(f\"Epoch {ep} 완료\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds, labs = [], []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, kg_vec, labels in loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            kg_vec = kg_vec.to(device)\n",
    "            logits = model(input_ids, attention_mask, kg_vec)\n",
    "            preds += torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            labs  += labels.tolist()\n",
    "    print(classification_report(labs, preds, target_names=[\"human\",\"ai\"], digits=4))\n",
    "\n",
    "# 7) 데이터로더 및 실험 실행\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "for use_kg in (False, True):\n",
    "    mode = \"KG 없이\" if not use_kg else \"KG 포함\"\n",
    "    print(f\"\\n[{mode}] 실험 시작\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        LateFusionDataset(train_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=True, num_workers=4, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        LateFusionDataset(test_df, use_kg=use_kg),\n",
    "        batch_size=16, shuffle=False, num_workers=4, pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = LateFusionModel(tfidf_dim)\n",
    "    train(model, train_loader, epochs=5)\n",
    "    print(f\"[{mode}] 평가 결과\")\n",
    "    evaluate(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
